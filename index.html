<html>

<head>
    <meta content="text/html; charset=UTF-8" http-equiv="content-type">
    <style type="text/css">
        @import url('https://themes.googleusercontent.com/fonts/css?kit=sDU-RIIs3Wq_4pUcDwWu-05zdwzqyXAFhQ3EpAK6bTA');
        ol.lst-kix_list_1-3 {
            list-style-type: none
        }
        
        ol.lst-kix_list_1-4 {
            list-style-type: none
        }
        
        ol.lst-kix_list_1-5 {
            list-style-type: none
        }
        
        ol.lst-kix_list_1-6 {
            list-style-type: none
        }
        
        ol.lst-kix_list_1-0 {
            list-style-type: none
        }
        
        .lst-kix_list_1-4>li {
            counter-increment: lst-ctn-kix_list_1-4
        }
        
        ol.lst-kix_list_1-1 {
            list-style-type: none
        }
        
        ol.lst-kix_list_1-2 {
            list-style-type: none
        }
        
        ol.lst-kix_list_1-6.start {
            counter-reset: lst-ctn-kix_list_1-6 0
        }
        
        .lst-kix_list_1-1>li {
            counter-increment: lst-ctn-kix_list_1-1
        }
        
        ol.lst-kix_list_1-3.start {
            counter-reset: lst-ctn-kix_list_1-3 0
        }
        
        ol.lst-kix_list_1-2.start {
            counter-reset: lst-ctn-kix_list_1-2 0
        }
        
        ol.lst-kix_list_1-8.start {
            counter-reset: lst-ctn-kix_list_1-8 0
        }
        
        .lst-kix_list_1-0>li:before {
            content: "" counter(lst-ctn-kix_list_1-0, decimal) ". "
        }
        
        ol.lst-kix_list_1-5.start {
            counter-reset: lst-ctn-kix_list_1-5 0
        }
        
        ol.lst-kix_list_1-7 {
            list-style-type: none
        }
        
        .lst-kix_list_1-1>li:before {
            content: "" counter(lst-ctn-kix_list_1-1, lower-latin) ". "
        }
        
        .lst-kix_list_1-2>li:before {
            content: "" counter(lst-ctn-kix_list_1-2, lower-roman) ". "
        }
        
        .lst-kix_list_1-7>li {
            counter-increment: lst-ctn-kix_list_1-7
        }
        
        ol.lst-kix_list_1-8 {
            list-style-type: none
        }
        
        .lst-kix_list_1-3>li:before {
            content: "" counter(lst-ctn-kix_list_1-3, decimal) ". "
        }
        
        .lst-kix_list_1-4>li:before {
            content: "" counter(lst-ctn-kix_list_1-4, lower-latin) ". "
        }
        
        ol.lst-kix_list_1-0.start {
            counter-reset: lst-ctn-kix_list_1-0 0
        }
        
        .lst-kix_list_1-0>li {
            counter-increment: lst-ctn-kix_list_1-0
        }
        
        .lst-kix_list_1-6>li {
            counter-increment: lst-ctn-kix_list_1-6
        }
        
        .lst-kix_list_1-7>li:before {
            content: "" counter(lst-ctn-kix_list_1-7, lower-latin) ". "
        }
        
        .lst-kix_list_1-3>li {
            counter-increment: lst-ctn-kix_list_1-3
        }
        
        .lst-kix_list_1-5>li:before {
            content: "" counter(lst-ctn-kix_list_1-5, lower-roman) ". "
        }
        
        .lst-kix_list_1-6>li:before {
            content: "" counter(lst-ctn-kix_list_1-6, decimal) ". "
        }
        
        ol.lst-kix_list_1-7.start {
            counter-reset: lst-ctn-kix_list_1-7 0
        }
        
        .lst-kix_list_1-2>li {
            counter-increment: lst-ctn-kix_list_1-2
        }
        
        .lst-kix_list_1-5>li {
            counter-increment: lst-ctn-kix_list_1-5
        }
        
        .lst-kix_list_1-8>li {
            counter-increment: lst-ctn-kix_list_1-8
        }
        
        ol.lst-kix_list_1-4.start {
            counter-reset: lst-ctn-kix_list_1-4 0
        }
        
        .lst-kix_list_1-8>li:before {
            content: "" counter(lst-ctn-kix_list_1-8, lower-roman) ". "
        }
        
        ol.lst-kix_list_1-1.start {
            counter-reset: lst-ctn-kix_list_1-1 0
        }
        
        ol {
            margin: 0;
            padding: 0
        }
        
        table td,
        table th {
            padding: 0
        }
        
        .c46 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 46.8pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c54 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 45.2pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c87 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 432pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c92 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 84.8pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c56 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 145.5pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c64 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 301.5pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c49 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 147pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c48 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 62.6pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c78 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 101.2pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c65 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 132.5pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c84 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 65.2pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c62 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 65.3pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c28 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 60.6pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c83 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 140.4pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c76 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 115.5pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c61 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 54.3pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c80 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 135.6pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c33 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 174pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c37 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 243.3pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c31 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 255.4pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c47 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 126.8pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c55 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 188.7pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c34 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 288.8pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c29 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 144pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c36 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 50.6pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c70 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 63.2pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c19 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 89.2pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c44 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 108.6pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c88 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 64.5pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c30 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 127.5pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c85 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 92.2pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c39 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 80.2pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c27 {
            border-right-style: solid;
            padding: 5pt 5pt 5pt 5pt;
            border-bottom-color: #000000;
            border-top-width: 1pt;
            border-right-width: 1pt;
            border-left-color: #000000;
            vertical-align: top;
            border-right-color: #000000;
            border-left-width: 1pt;
            border-top-style: solid;
            border-left-style: solid;
            border-bottom-width: 1pt;
            width: 129.8pt;
            border-top-color: #000000;
            border-bottom-style: solid
        }
        
        .c68 {
            margin-left: 36pt;
            padding-top: 0pt;
            padding-left: 0pt;
            padding-bottom: 6pt;
            line-height: 1.15;
            orphans: 2;
            widows: 2;
            text-align: justify
        }
        
        .c57 {
            margin-left: 36pt;
            padding-top: 6pt;
            padding-left: 0pt;
            padding-bottom: 0pt;
            line-height: 1.15;
            orphans: 2;
            widows: 2;
            text-align: justify
        }
        
        .c9 {
            color: #171717;
            font-weight: 400;
            text-decoration: none;
            vertical-align: baseline;
            font-size: 24pt;
            font-family: "Times New Roman";
            font-style: normal
        }
        
        .c3 {
            color: #171717;
            font-weight: 400;
            text-decoration: none;
            vertical-align: baseline;
            font-size: 18pt;
            font-family: "Times New Roman";
            font-style: normal
        }
        
        .c52 {
            padding-top: 12pt;
            padding-bottom: 6pt;
            line-height: 1.15;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: center
        }
        
        .c2 {
            color: #171717;
            font-weight: 400;
            text-decoration: none;
            vertical-align: baseline;
            font-size: 12pt;
            font-family: "Times New Roman";
            font-style: normal
        }
        
        .c45 {
            padding-top: 2pt;
            padding-bottom: 0pt;
            line-height: 1.15;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: justify
        }
        
        .c16 {
            color: #171717;
            font-weight: 400;
            text-decoration: none;
            vertical-align: baseline;
            font-size: 22pt;
            font-family: "Times New Roman";
            font-style: normal
        }
        
        .c6 {
            padding-top: 6pt;
            padding-bottom: 3pt;
            line-height: 1.15;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: center
        }
        
        .c8 {
            color: #171717;
            font-weight: 400;
            text-decoration: none;
            vertical-align: baseline;
            font-size: 14pt;
            font-family: "Times New Roman";
            font-style: normal
        }
        
        .c35 {
            padding-top: 2pt;
            padding-bottom: 0pt;
            line-height: 1.15;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: left
        }
        
        .c58 {
            color: #3b3838;
            font-weight: 700;
            text-decoration: none;
            vertical-align: baseline;
            font-size: 24pt;
            font-family: "Times New Roman";
            font-style: normal
        }
        
        .c20 {
            color: #171717;
            font-weight: 700;
            text-decoration: none;
            vertical-align: baseline;
            font-size: 12pt;
            font-family: "Times New Roman";
            font-style: normal
        }
        
        .c63 {
            padding-top: 0pt;
            padding-bottom: 10pt;
            line-height: 1.15;
            orphans: 2;
            widows: 2;
            text-align: left
        }
        
        .c74 {
            padding-top: 0pt;
            padding-bottom: 0pt;
            line-height: 1.0;
            orphans: 2;
            widows: 2;
            text-align: left
        }
        
        .c5 {
            padding-top: 6pt;
            padding-bottom: 6pt;
            line-height: 1.15;
            orphans: 2;
            widows: 2;
            text-align: center
        }
        
        .c81 {
            padding-top: 0pt;
            padding-bottom: 8pt;
            line-height: 1.15;
            orphans: 2;
            widows: 2;
            text-align: center
        }
        
        .c17 {
            padding-top: 0pt;
            padding-bottom: 8pt;
            line-height: 1.15;
            orphans: 2;
            widows: 2;
            text-align: left
        }
        
        .c1 {
            padding-top: 6pt;
            padding-bottom: 6pt;
            line-height: 1.15;
            orphans: 2;
            widows: 2;
            text-align: justify
        }
        
        .c50 {
            padding-top: 6pt;
            padding-bottom: 6pt;
            line-height: 1.15;
            orphans: 2;
            widows: 2;
            text-align: left
        }
        
        .c73 {
            padding-top: 0pt;
            padding-bottom: 0pt;
            line-height: 1.15;
            orphans: 2;
            widows: 2;
            text-align: justify
        }
        
        .c10 {
            padding-top: 0pt;
            padding-bottom: 10pt;
            line-height: 1.15;
            orphans: 2;
            widows: 2;
            text-align: center
        }
        
        .c93 {
            color: #434343;
            text-decoration: none;
            vertical-align: baseline;
            font-size: 24pt;
            font-style: normal
        }
        
        .c7 {
            padding-top: 0pt;
            padding-bottom: 0pt;
            line-height: 1.15;
            text-align: left
        }
        
        .c22 {
            color: #171717;
            text-decoration: none;
            vertical-align: baseline;
            font-style: normal
        }
        
        .c71 {
            margin-left: 25.2pt;
            border-spacing: 0;
            border-collapse: collapse;
            margin-right: auto
        }
        
        .c12 {
            background-color: #ffffff;
            font-size: 14pt;
            font-family: "Times New Roman";
            font-weight: 400
        }
        
        .c23 {
            padding-top: 0pt;
            padding-bottom: 0pt;
            line-height: 1.15;
            text-align: center
        }
        
        .c41 {
            margin-left: 46.1pt;
            border-spacing: 0;
            border-collapse: collapse;
            margin-right: auto
        }
        
        .c43 {
            margin-left: auto;
            border-spacing: 0;
            border-collapse: collapse;
            margin-right: auto
        }
        
        .c75 {
            font-weight: 700;
            font-size: 16pt;
            font-family: "Times New Roman"
        }
        
        .c77 {
            font-weight: 400;
            font-size: 12pt;
            font-family: "Cambria"
        }
        
        .c11 {
            font-family: "Times New Roman";
            font-style: italic;
            font-weight: 400
        }
        
        .c18 {
            font-family: "Times New Roman";
            color: #171717;
            font-weight: 400
        }
        
        .c51 {
            font-weight: 700;
            font-size: 26pt;
            font-family: "Times New Roman"
        }
        
        .c13 {
            font-size: 14pt;
            font-family: "Times New Roman";
            font-weight: 700
        }
        
        .c90 {
            padding: 0;
            margin: 0
        }
        
        .c21 {
            font-weight: 400;
            font-family: "Times New Roman"
        }
        
        .c72 {
            max-width: 468pt;
            padding: 72pt 72pt 72pt 72pt
        }
        
        .c4 {
            height: 12pt
        }
        
        .c38 {
            font-size: 11pt
        }
        
        .c79 {
            height: 27pt
        }
        
        .c24 {
            font-size: 36pt
        }
        
        .c26 {
            font-size: 14pt
        }
        
        .c0 {
            height: 28pt
        }
        
        .c59 {
            height: 16pt
        }
        
        .c25 {
            font-size: 22pt
        }
        
        .c40 {
            margin-right: 18pt
        }
        
        .c53 {
            height: 0pt
        }
        
        .c32 {
            font-size: 30pt
        }
        
        .c60 {
            vertical-align: super
        }
        
        .c67 {
            height: 18pt
        }
        
        .c42 {
            background-color: #ffffff
        }
        
        .c69 {
            height: 17pt
        }
        
        .c15 {
            height: 22pt
        }
        
        .c89 {
            height: 26pt
        }
        
        .c86 {
            height: 23pt
        }
        
        .c66 {
            page-break-after: avoid
        }
        
        .c91 {
            height: 1pt
        }
        
        .c14 {
            font-style: italic
        }
        
        .c82 {
            height: 14pt
        }
        
        .title {
            padding-top: 6pt;
            color: #3b3838;
            font-weight: 700;
            font-size: 24pt;
            padding-bottom: 3pt;
            font-family: "LM Roman 12";
            line-height: 1.0;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: center
        }
        
        .subtitle {
            padding-top: 18pt;
            color: #666666;
            font-size: 24pt;
            padding-bottom: 4pt;
            font-family: "Georgia";
            line-height: 1.1500000000000001;
            page-break-after: avoid;
            font-style: italic;
            orphans: 2;
            widows: 2;
            text-align: justify
        }
        
        li {
            color: #171717;
            font-size: 12pt;
            font-family: "Cambria"
        }
        
        p {
            margin: 0;
            color: #171717;
            font-size: 12pt;
            font-family: "Cambria"
        }
        
        h1 {
            padding-top: 12pt;
            color: #171717;
            font-weight: 700;
            font-size: 26pt;
            padding-bottom: 6pt;
            font-family: "Cambria";
            line-height: 1.1500000000000001;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: center
        }
        
        h2 {
            padding-top: 2pt;
            color: #171717;
            font-weight: 700;
            font-size: 16pt;
            padding-bottom: 0pt;
            font-family: "Cambria";
            line-height: 1.1500000000000001;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: justify
        }
        
        h3 {
            padding-top: 2pt;
            color: #171717;
            font-weight: 700;
            font-size: 14pt;
            padding-bottom: 0pt;
            font-family: "Cambria";
            line-height: 1.1500000000000001;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: justify
        }
        
        h4 {
            padding-top: 12pt;
            color: #171717;
            font-weight: 700;
            font-size: 12pt;
            padding-bottom: 2pt;
            font-family: "Cambria";
            line-height: 1.1500000000000001;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: justify
        }
        
        h5 {
            padding-top: 11pt;
            color: #171717;
            font-weight: 700;
            font-size: 11pt;
            padding-bottom: 2pt;
            font-family: "Cambria";
            line-height: 1.1500000000000001;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: justify
        }
        
        h6 {
            padding-top: 10pt;
            color: #171717;
            font-weight: 700;
            font-size: 10pt;
            padding-bottom: 2pt;
            font-family: "Cambria";
            line-height: 1.1500000000000001;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: justify
        }

        .main {
            position: absolute;
            left: 50%;
            transform: translateX(-50%)
        }
    </style>
</head>

<body class="c42 c72">
    <div class="main">
    <div>
        <p class="c4 c74"><span class="c22 c77"></span></p>
    </div>
    <p class="c23"><span class="c22 c21 c24">A Framework for Sensor-Based Hand Gesture Classification using Machine Learning<br><br></span></p>
    <p class="c4 c23"><span class="c22 c21 c24"></span></p>
    <p class="c23 c4"><span class="c22 c21 c24"></span></p>
    <p class="c23 c4"><span class="c22 c21 c24"></span></p>
    <p class="c23 c4"><span class="c22 c21 c24"></span></p>
    <p class="c7 c4"><span class="c22 c21 c38"></span></p>
    <p class="c7 c4"><span class="c22 c21 c38"></span></p>
    <p class="c7 c4"><span class="c22 c21 c38"></span></p>
    <p class="c6 title" id="h.3dy6vkm"><span class="c18">Chapter 1</span></p>
    <h1 class="c52" id="h.1t3h5sf"><span class="c21 c32">Introduction</span></h1>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <h2 class="c35" id="h.4d34og8"><span class="c9">1.1 Motivation and goal</span></h2>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c8">In this world of computers, machines have become an integral part of human day to day work. A field of device designing has been introduced named Human Computer Interaction (HCI). Human-Computer Interaction (HCI) is a multidisciplinary field of study focusing on the design of computer technology and, in particular, the interaction between humans (the users) and computers. While initially concerned with computers, HCI has since expanded to cover almost all forms of information technology design [1]. Under this field, human behavior and machine control researches have been conducted extensively and such devices are being developed more often than ever. </span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c8">Human interaction is based upon communication systems. It enables us to exchange ideas, feelings and emotions towards other human beings. This communication is done not only when we speak to one another but can also be formulated and transferred when we walk, run, play or even sit together. Human communication of interactions can be classified into two categories: verbal and non-verbal. The verbal communication is the combination of speaking and listening in a language mutually understandable. It can be done face to face with another person or even via phone, radio, television etc. The non-verbal way of communication consists of a person&rsquo;s body language, gestures, postures and even with writings or pictures.</span></p>
    <p class="c1"><span class="c8">&ldquo;Gesture&rdquo; means the movement of different limbs of body such as - hands, head, arm, body and even facial expression. This is very common form to express ideas, emotions and opinions. &ldquo;Recognition&rdquo; denotes to the identification and classification of different objects. So gesture recognition is the act of identifying human gestures. A computer based system with gesture recognition capability can recognize human gestures and perform certain commands or tasks accordingly [2].</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c8">With the help of development in machine learning algorithms and various tools, many companies like Google, Amazon have designed their products compatible to natural language. A very common of natural language is hand gestures which have been common in societies all over the world regardless of language and cast. There are fundamental differences in sign languages because of the language itself. But some basic hand gestures have always been common in different societies. </span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c8">In this project we aim to develop a sensor based gesture recognition framework which can detect human hand gesture in real time and also implement this framework in HCI and automation. We aim to classify 14 different hand gestures from the data of three different sensors (flex sensor, 3-axis accelerometer, 3-axis gyroscope). Such gesture recognition is a vital step for controlling several computer based utilities.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <h2 class="c45" id="h.2s8eyo1"><span class="c9">1.2 Approaches to hand gesture recognition</span></h2>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21 c26">To design a HCI based system, we have to dig into the methods of detecting human communication methods. Also, in several scenarios, the design is based on different activity of human which is part of another field of study named Human Activity Recognition (HAR). While HCI design can have different approaches, gesture detection can have the basic two approaches</span><span class="c21">.</span>
        <hr style="page-break-before:always;display:none;">
    </p>
    <h3 class="c45" id="h.17dp8vu"><span class="c18 c25">1.2.1</span><span class="c21 c25">&nbsp;</span><span class="c16">Computer vision based system</span></h3>
    <p class="c1 c4"><span class="c22 c77"></span></p>
    <p class="c1"><span class="c8">This field has been improved drastically in past few years. In vision based approach, the users do not need to wear any extra devices. This technique is very natural to use for the users because human and computer will communicate directly [2]. Hand images are captured by the cameras and using the vision based techniques, captured images can be processed and analyzed. Different types of cameras are used: monocular, fish eye, time of flight and infrared cameras etc. [3] The representations of alphabets and numbers are easily recognized with the vision based approach. The illumination change, background cluster, partial or full occlusions are the several challenges to be addressed.</span></p>
    <p class="c1" id="h.3rdcrjn"><span class="c21 c25"><br></span><span class="c18 c25">1.2.2</span><span class="c21 c25">&nbsp;</span><span class="c18 c25">Sensor based system</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c8">Sensor based systems for hand gesture recognition are popular nowadays due to the complication of camera based systems in activity recognition. There are mainly two types of sensor based systems as shown below.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c13">(i) Dedicated device:</span><span class="c8">&nbsp;This type of devices are action based. For example, a data glove or a gaming remote. These devices are dedicated to that particular tasks they were meant to do. For example a data glove can contain several sensors based on the use and this gloves are very accurate to perform the task for example certain hand gestures detection. But to use this gloves in any other task, certain level of modification is required. Same for a gaming remote. This has been very popular in TV gaming consoles. Users can make certain game commands holding the remote in their hand and making gestures with it.</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c13">(ii) General purpose device:</span><span class="c8">&nbsp;This type of devices generally have wide range of utilities. For example, smartphones and smartwatches are such devices. They are not dedicated to a particular tasks. Rather they can be programmed to do various tasks based on available sensors. The differences between these two varieties are based on the number of sensors and utilities. For example, an Apple smart watch can be used as a digital watch and along with showing time, it can perform certain movement detection as it contains sensors to do so. But a data-gloves or a gaming remotes are designed to perform a particular task but cannot be used all day for any general purpose work. </span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <h2 class="c35" id="h.26in1rg"><span class="c9">1.3 Challenges in sensor based gesture recognition</span></h2>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c8">There are several challenges to consider before applying sensor based gesture recognition method. These challenges are discussed in the following sections.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <h3 class="c35" id="h.lnxbz9"><span class="c16">1.3.1 Type of gesture</span></h3>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c8">There are mainly two types of hand gestures based on detection mechanism:</span></p>
    <p class="c1"><span class="c21 c26"><br></span><span class="c13">(i) Static Gesture:</span><span class="c8">&nbsp;This type of hand gestures are based on static orientation of hand. For example, to express &lsquo;Stop&rsquo; we generally show palm of our hands vertically. The static position of the hand expresses the person in front to stop. Static gestures are comparatively easier to detect as over time the sensors show nearly similar values.</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c13">(ii) Dynamic Gestures:</span><span class="c8">&nbsp;This type of hand gestures are based on certain hand movements. For example, waving a hand attracts the viewer&rsquo;s attention. Dynamic hand gestures are relatively more complex to detect. Here, several time windows must be used to cut the relevant portion of the sensor signal as gestures. For different users this window varies and so alongside the raw data, different time domain, frequency domain and temporal domain data must be considered. The window for the gesture must be taken according to the continuous gesture length. The data processing in this type is more complex. Various real life gestures are of this kind.</span></p>
    <p class="c1"><span class="c8">So, here the challenge is to develop algorithm to work with any single type of hand gestures or combine the both detection method to work with both type of gestures. </span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <h3 class="c35" id="h.35nkun2"><span class="c16">1.3.2 Appropriate sensors</span></h3>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c8">There are different types of sensors available for detecting hand gestures. Another challenge is to select the appropriate sensor or sensors for detecting the selected gestures successfully. For detecting hand gestures, there are several sensors which detect various features. For example, flex sensors can detect bend angles based on resistive change. Accelerometer can detect acceleration is different axis. Gyroscope can detect angular velocity along with different axis. Moreover, magnetometer can also be used in different cases to detect orientation based on earth&rsquo;s magnetic field values. The last three sensors are sometimes used together to detect the orientation of an object in 3D plane. Another way of measuring hand gesture is to use EMG sensors. EMG sensors can detect muscle signals based on different hand gestures. So, a big challenge is to select appropriate sensor based on the type of hand gesture and the application.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <h3 class="c35" id="h.1ksv4uv"><span class="c16">1.3.3 Dataset</span></h3>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c8">Choosing and working with appropriate dataset is another challenge. There are many datasets available online for free in different website. For example: UCI HAR dataset, UCI HAPT Dataset etc. These online available datasets are generally made with data collected in lab environment. So, the dataset chosen to work with must have diversity. That means different age groups and genders must participate in the dataset making.</span></p>
    <p class="c1"><span class="c8"><br>Another approach can be making our own dataset using a certain device. This approach is generally preferred for a rather non-conventional devices like data glove. But making a dataset is a tough job as we need to assemble participants from different age groups, genders to make a diversified dataset. Another important factor is the number of participants. A good number of participants are needed to create a good and reliable dataset but the number is not defined; the more the better. The data must be taken by a stable device under stable condition from the participants. Moreover, before actually using these data to train a model, data must be pre-processed.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <h3 class="c35" id="h.44sinio"><span class="c16">1.3.4 Appropriate training model</span></h3>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c8">Finding appropriate training model is another challenge. There are a number of training models to use. For example, linear regression, logistic regression, support vector machine (SVM), K-nearest neighbors (KNN), decision trees, random forests (RnF) etc. Finding appropriate model well fitted to show good results is very important. </span></p>
    <p class="c1"><span class="c8">In a famous paper published in 2001, Microsoft researchers Michele Banko and Eric Brill showed that very different Machine Learning algorithms, including fairly simple ones, performed almost identically well on a complex problem of natural language disambiguation once they were given enough data [4]. </span></p>
    <p class="c1"><span class="c8">Generally, such large and reliable datasets are not available from every type of work. So, algorithms play a vital role in achieving good prediction. There is no definite way to say beforehand that a certain algorithm will perform best for a dataset. This can be determined after tuning various algorithms and comparing the results.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <hr style="page-break-before:always;display:none;">
    <p class="c17 c4" id="h.2jxsxqh"><span class="c22 c75"></span></p>
    <h2 class="c45" id="h.z337ya"><span class="c9">1.4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Project overview</span></h2>
    <p class="c1 c4"><span class="c22 c77"></span></p>
    <p class="c1"><span class="c8">Chapter 1 provides a brief introduction to the concepts of human computer interaction systems, where intentions and motivation of this research are clarified. &nbsp;Hand gesture detection approaches have been shown with pros and cons. Moreover, various challenges in this research has also been discussed. Finally, an overview of basic requirements of this project has been given.</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c8">Chapter 2 discusses other studies and related works in the area of hand gesture recognition. Firstly, it describes dedicated device based machine learning techniques. Secondly, a comparison of different approaches proposed by various researchers along with accuracies of various sensors has been shown for data glove of similar form. Finally, problems related to previous works have been discussed.</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c8">Chapter 3 provides a theoretical overview and work flow of this project work including sensor description, choice of sensor type and position, etc. Different approaches solving our gesture detection problems and their comparison are also shown. The reasons for adapting different approaches also explained. Following this, types of features and several machine learning techniques were discussed in brief.</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c8">Chapter 4 discusses our research methodology. Firstly a basic structure of the system has been given. Secondly, feature extraction and selection technique have been described. Finally, our proposed method was specified for the precise classification of 14 different hand gestures are shown.</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c8">Chapter 5 provides the results obtained from the investigation of our proposed method. A comparison is also given showing the accuracy of our proposed method with traditional classifiers. Finally, a discussion and detailed analysis of our proposed method have been given.</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c21 c26">Chapter 6 draws conclusion of this work and provides future improvement scopes in this research.</span>
        <hr style="page-break-before:always;display:none;">
    </p>
    <p class="c6 c4 title"><span class="c9"></span></p>
    <p class="c6 c4 title"><span class="c9"></span></p>
    <p class="c6 title"><span class="c18">Chapter 2</span></p>
    <h1 class="c52" id="h.3j2qqm3"><span class="c21 c32">Related work</span></h1>
    <p class="c1 c4"><span class="c2"><br></span></p>
    <p class="c1"><span class="c21 c26">M&auml;ntyj&auml;rvi </span><span class="c11 c26">et al</span><span class="c8">.[5] have used accelerometer sensor to recognize DVD player gesture. They focused on discrete gesture command. Discrete Hidden Markov Models are applied here. For a set of eight gestures, each trained with two original gestures and with four noise-distorted duplicates, the average recognition accuracy was 98%, cross-validated from a total data set of 240 gestures. A device with 3 axis accelerometer is used to capture the gesture. The device is connected to a computer via serial communication. The computer recognizes the signal and outputs certain action. They used a certain button to record the discrete signals.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c5 c66"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 423.75px; height: 290.51px;"><img alt="https://lh4.googleusercontent.com/heLgecDvzrrINc6tWHaZ7crKNUemExSEHjAXWF9C1LDcUo4_AZUgsz_imeiAnKTGRm3JKOJ28eHzP5g2QIH4pioxT8T7ZEiCz14Dw-JCjwxv66BEWIAZMvZbWMKpkx-00coersuP" src="images/image38.png" style="width: 423.75px; height: 290.51px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c10" id="h.1y810tw"><span class="c8">Figure 1.1: Eight different DVD player control gestures</span></p>
    <p class="c1"><span class="c21 c26">According to the work of Schl&ouml;mer </span><span class="c11 c26">et al</span><span class="c8">.[6], a accelerometer based gaming remote (wiimote) is used to recognize 5 gaming gestures separately. There were 6 participants; each performed the five gestures 15 times each. The results for the five gestures were Square = 88.8%, Circle = 86.6%, Roll = 84.3%, Z = 94.3%, and Tennis = 94.5%. The process of recognition contains filtering, quantization, HMM and Bayes classifier.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c5 c66"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 395.87px; height: 97.33px;"><img alt="https://lh4.googleusercontent.com/KULVWrf-tVKKHDHHn-FB46DBISAiea2Mey97sfrtH1H2Mb_ke10hK9V9ppQ0UjQPdOZ914GK5PiS09ZZo1i7bQ-olkKYqRpHdo_ZR6w30hhr9dMEgI-_dqn5Tuw2s_MTo-ILNyfH" src="images/image37.png" style="width: 395.87px; height: 97.33px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c10" id="h.4i7ojhp"><span class="c8">Figure 1.2: Five reference gaming gestures</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c21 c26">In the work of Roy </span><span class="c11 c26">et al</span><span class="c8">.[7] a technique of combining electromyography and accelerometer data is used to predict hand gestures. They worked on two movements - palm open and close. MPU6050 accelerometer sensor and used a custom-made EMG sensor along with a microcontroller (atmega328p) are used to capture and relay the signals via Bluetooth to the computer.</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c21 c26">Zhang </span><span class="c11 c26">et al</span><span class="c8">.[8] have used a fusion of a three-axis accelerometer (ACC) and multichannel electromyography (EMG) sensors. The start and end points of meaningful gesture segments are detected automatically by the intensity of the EMG signals. A decision tree and multi-stream hidden Markov models are utilized as decision-level fusion to get the final results. </span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c5 c66"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 623.33px; height: 220.93px;"><img alt="https://lh3.googleusercontent.com/OW6nScVfhKienLUEAftiOEjrNw-vUhCHY3Fqp6BPNcDCtfq480gUxArsuqO0ZY9rZ7RiDTSEvb9o4-51VRk7W3yxIlLuuzMXgq5RcwjZ7QeUl8XePfBtAJM8yv5PB5DrX5hK6sXo" src="images/image40.png" style="width: 623.33px; height: 220.93px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c10" id="h.2xcytpi"><span class="c8">Figure 1.3: Block diagram of the proposed framework by Zhang et al</span><span class="c2">.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c8">For sign language recognition (SLR), experimental results on the classification of 72 Chinese Sign Language words demonstrate the complementary functionality of the ACC and EMG sensors and the effectiveness of this framework. The overall recognition accuracy was 90.2%.</span></p>
    <p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 421.50px; height: 331.05px;"><img alt="" src="images/image39.png" style="width: 421.50px; height: 331.05px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c10" id="h.1ci93xb"><span class="c8">Figure 1.4: 3D accelerometer and EMG sensor placement</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c21 c26">In the work of Amma </span><span class="c11 c26">et al</span><span class="c8">.[9] three gyroscopes and three accelerometers are used to measure hand motion. Data is sent wirelessly to the computer via Bluetooth. HMMs are used for character recognition and concatenated character models are used for word recognition. As features normalized raw sensor signals are applied. They used discrete gestures. The text is written into the air via character gestures. A wearable input device (glove) is used which enables the user to input text into a computer. An average writer dependent character recognition rate of 94.8% and a writer independent character recognition rate of 81.9%.</span></p>
    <p class="c1"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p>
    <p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 393.20px; height: 244.60px;"><img alt="https://lh3.googleusercontent.com/zqhYJ-Bo2IHpj9b1eJqw6oGaZM9SKOxQPRmQPxkTZYeHYGX1D0ywEzqgBoAJtUVTlwL1xDCm9RVEO7KcbxozAsHIVp4Y_YTfDlEc2ZhhAOoAT_Bg68qekw94uY9q48Zx1QI-M4zh" src="images/image42.png" style="width: 393.20px; height: 244.60px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c10" id="h.3whwml4"><span class="c8">Figure 1.5: Data glove used in air-writing.<br></span></p>
    <p class="c1"><span class="c21 c26">Pittman </span><span class="c11 c26">et al</span><span class="c8">.[10] this paper used Doppler effect in sound wave to recognize two types of single hand gestures, swipes and taps, in four directions: left, right, towards the monitor, and away from the monitor; in total 8 gestures. </span></p>
    <p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 175.41px; height: 100.86px;"><img alt="https://lh4.googleusercontent.com/ekVdcbjZX2FFZ6TPqysqx2z5y_PUwmaTXOI8SUMDuTJDajPu_kcIdTDGF8eP9TggtPU1_vdwkXtk5HsNiDGGXAaZTUeuMDSXVzbjzTHguAm6EO565Aqy2K_tFpXMFKESKUipuqjn" src="images/image41.png" style="width: 175.41px; height: 100.86px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c10" id="h.2bn6wsx"><span class="c8">Figure 1.6: Gestures used in Doppler Effect based detection.</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c8">They used two microphones and more than two speakers to capture the reflected sound. To segment gestures, we looked at periods of motion that stopped for 200 ms. To prevent any meaningless input, we set a minimum gesture duration at 150 ms. The features extracted from the detected gestures passed through a Random Forests classifier generated using WEKA. The overall accuracy was 93.8%.</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c21 c26">Uddin </span><span class="c11 c26">et al</span><span class="c8">.[11] used skin detection algorithm is used to detect the hand correctly. The model detects the skin color of every type using &#x1d44c;&#x1d436;&#x1d435;&#x1d436;&#x1d445; algorithm, Bag of words model is used for feature extraction and Support Vector Machine (SVM) for training and evaluation. To validate the model, a custom made dataset is made. The average accuracy is 86%.</span></p>
    <hr style="page-break-before:always;display:none;">
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c6 c4 title"><span class="c9"></span></p>
    <p class="c6 title"><span class="c18">Chapter 3</span></p>
    <h1 class="c52" id="h.qsh70q"><span class="c21 c32">Theoretical Background</span></h1>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c8">This project addresses a method to correctly classify human hand gestures using accelerometer, gyroscope and flex sensors. The accelerometer sensor measures acceleration in three axis (x, y, z). The gyroscope sensor can measure angular velocity in three axis (x, y, z). Both accelerometer and gyroscope sensor data have been used to calculate the three dimensional orientation of hand. The flex sensors are used to measure the bending of the fingers. A microcontroller placed on the glove interfaces with these sensors, collects data from them sends the data to a computer through the serial interface. All the sensors are placed on the data collection glove in a manner so that they can provide measurement on both 3D orientation of the hand and finger bending.</span></p>
    <p class="c1 c4"><span class="c2"><br></span></p>
    <h2 class="c45" id="h.3as4poj"><span class="c9">3.1 Sensor Description</span></h2>
    <p class="c1 c4"><span class="c2"></span></p>
    <h3 class="c45" id="h.1pxezwc"><span class="c16">3.1.1 MPU-6050 (Accelerometer and Gyroscope)</span></h3>
    <p class="c1"><span class="c2">&nbsp;&nbsp;&nbsp; </span></p>
    <p class="c1"><span class="c8">The MPU-6050 is an integrated 6 axis motion tracking device that combines a 3-axis Accelerometer, 3-axis Gyroscope and a Digital Motion Processor (DMP). The sensor can also accept output from an external 3-axis Compass sensor or Magnetometer to provide a 9-axis Motion Fusion output. Hence the MPU-6050 is also called a 9-DOF sensor [12]. </span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 128.00px; height: 177.43px;"><img alt="" src="images/image45.png" style="width: 128.00px; height: 177.43px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c10" id="h.49x2ik5"><span class="c8">Figure 3.1: Pinout of MPU-6050</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c8">Communication with all registers of the device is performed using I2C interface at 400 kHz. Table 3.1 adapted from the MPU-6050 datasheet provides the basic information about the sensor [13].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c10" id="h.2p2csry"><span class="c8">Table 3.1: MPU-6050 Specifications</span></p>
    <a id="t.4d654bd24b4b44b007d6dd9bfd06c3ccc4107da7"></a>
    <a id="t.0"></a>
    <table class="c41">
        <tbody>
            <tr class="c53">
                <td class="c29" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">Parameters</span></p>
                </td>
                <td class="c34" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">Values</span></p>
                </td>
            </tr>
            <tr class="c53">
                <td class="c29" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">Operating Voltage</span></p>
                </td>
                <td class="c34" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">2.375V - 3.46V</span></p>
                </td>
            </tr>
            <tr class="c53">
                <td class="c29" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">Serial Interface</span></p>
                </td>
                <td class="c34" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">I2C</span></p>
                </td>
            </tr>
            <tr class="c53">
                <td class="c29" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">ADC Resolution</span></p>
                </td>
                <td class="c34" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">16 bits</span></p>
                </td>
            </tr>
            <tr class="c53">
                <td class="c29" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">Accelerometer ranges</span></p>
                </td>
                <td class="c34" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">&plusmn;2g, &plusmn;4g, &plusmn;8g, and &plusmn;16g</span></p>
                </td>
            </tr>
            <tr class="c69">
                <td class="c29" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">Gyroscope ranges</span></p>
                </td>
                <td class="c34" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">&plusmn;250, &plusmn;500, &plusmn;1000, and &plusmn;2000&deg;/sec (dps)</span></p>
                </td>
            </tr>
        </tbody>
    </table>
    <p class="c1 c4"><span class="c20"></span></p>
    <p class="c1"><span class="c13">(i) The Accelerometer</span><span class="c21 c26">: The MPU-6050 contains a 3-axis 16 bit accelerometer sensor. Consistent with Newton&rsquo;s second law of motion, it measures the acceleration force in </span><img src="images/image1.png"><span class="c21 c26">&nbsp;that is applied to a device on all three physical axes (x, y, and z) including the force of gravity, where</span><img src="images/image2.png"><span class="c8">. &nbsp;The fundamental principle of operation behind the accelerometer based on Microelectromechanical Systems (MEMS) is the displacement of a small proof mass etched into the silicon surface of the integrated circuit and suspended by small beams[39].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c8">The on board accelerometer of the sensor is configured by writing to the ACCEL_CONFIG (Address: 1C) register shown in Table 3.2. One important parameter of the accelerometer is the full scale range of the sensor. Full scale range can be set from &plusmn;2g to &plusmn;16g by writing to the AFS_SEL bit of the configuration register (ACCEL_CONFIG). Lower range gives higher sensitivity and higher range gives lower sensitivity. AFS_SEL selects the full scale range of the accelerometer outputs according to Table 3.3.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c10" id="h.147n2zr"><span class="c8">Table 3.2: ACCEL_CONFIG Register</span></p>
    <a id="t.a19ac7ab5231d2a674d98724784fe6afd69e7001"></a>
    <a id="t.1"></a>
    <table class="c43">
        <tbody>
            <tr class="c89">
                <td class="c48" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">Bit 7</span></p>
                </td>
                <td class="c48" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">Bit 6</span></p>
                </td>
                <td class="c28" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">Bit 5</span></p>
                </td>
                <td class="c61" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">Bit 4</span></p>
                </td>
                <td class="c61" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">Bit 3</span></p>
                </td>
                <td class="c54" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">Bit 2</span></p>
                </td>
                <td class="c54" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">Bit 1</span></p>
                </td>
                <td class="c54" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">Bit 0</span></p>
                </td>
            </tr>
            <tr class="c89">
                <td class="c48" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">XA_ST</span></p>
                </td>
                <td class="c48" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">YA_ST</span></p>
                </td>
                <td class="c28" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">ZA_ST</span></p>
                </td>
                <td class="c44" colspan="2" rowspan="1">
                    <p class="c5"><span class="c8">AFS_SEL [1:0]</span></p>
                </td>
                <td class="c80" colspan="3" rowspan="1">
                    <p class="c5"><span class="c8">-</span></p>
                </td>
            </tr>
        </tbody>
    </table>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c10 c4" id="h.3o7alnk"><span class="c2"></span></p>
    <p class="c10"><span class="c8">Table 3.3: Full Scale Range Selection and Sensitivity</span></p>
    <a id="t.76f179a89cd0dd45a53fd5f32893d2553c12949b"></a>
    <a id="t.2"></a>
    <table class="c71">
        <tbody>
            <tr class="c53">
                <td class="c47" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">AFS_SEL</span></p>
                </td>
                <td class="c56" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">Full Scale Range</span></p>
                </td>
                <td class="c49" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">LSB Sensitivity</span></p>
                </td>
            </tr>
            <tr class="c53">
                <td class="c47" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">0</span></p>
                </td>
                <td class="c56" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">&plusmn;2g</span></p>
                </td>
                <td class="c49" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">16384 LSB/g</span></p>
                </td>
            </tr>
            <tr class="c53">
                <td class="c47" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">1</span></p>
                </td>
                <td class="c56" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">&plusmn;4g</span></p>
                </td>
                <td class="c49" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">8192 LSB/g</span></p>
                </td>
            </tr>
            <tr class="c53">
                <td class="c47" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">2</span></p>
                </td>
                <td class="c56" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">&plusmn;8g</span></p>
                </td>
                <td class="c49" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">4096 LSB/g</span></p>
                </td>
            </tr>
            <tr class="c53">
                <td class="c47" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">3</span></p>
                </td>
                <td class="c56" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">&plusmn;16g</span></p>
                </td>
                <td class="c49" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">2048 LSB/g</span></p>
                </td>
            </tr>
        </tbody>
    </table>
    <p class="c5 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21">&nbsp;&nbsp;&nbsp; </span><span class="c8">The most recent measurements of the accelerometer are stored in ACCEL_XOUT_H, ACCEL_XOUT_L, ACCEL_YOUT_H, ACCEL_YOUT_L, ACCEL_ZOUT_H, and ACCEL_ZOUT_L registers (Address: 3B - 40). The data within the accelerometer sensors&rsquo; internal register set is always updated at the Sample Rate selected. To get the actual measurement these 16 bit data needs to be divided by the LSB Sensitivity corresponding to the selected Full Scale Range. If a Full Scale range of &plusmn;2g is selected then the actual accelerometer reading is given by the following equation [20].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c5"><img src="images/image3.png"></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c13">(ii) Gyroscope: </span><span class="c8">The MPU-6050 contains a 3-axis 16 bit gyroscope sensor. A gyroscope identifies up/down, left/right and rotation around three axes (x, y, z) for more complex orientation details. It can measure angular velocity of the sensor in three axis. By integrating the angular velocity with respect to time gives the angular displacement. The gyroscopes inside MPU-6050 don&rsquo;t use wheels and gimbals like the traditional mechanical ones. Instead, they are MEMS (Micro-Electro-Mechanical Systems) gyroscopes, a smaller version of the concept embedded on an electronics board. </span></p>
    <p class="c1"><span class="c8">The on board gyroscope of the sensor is configured by writing to the GYRO_CONFIG (Address: 1B) register shown in Table 3.2. Like the accelerometer an important parameter of the gyroscope is the full scale range of the sensor. Full scale range can be set from &plusmn;250&deg;/sec to &plusmn;2000&deg;/sec by writing to the FS_SEL bit of the configuration register (GYRO_CONFIG). Lower range gives higher sensitivity and higher range gives lower sensitivity. FS_SEL selects the full scale range of the gyroscope outputs according to Table 3.4.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c10" id="h.23ckvvd"><span class="c8">Table 3.4: GYRO_CONFIG Register</span></p>
    <a id="t.8485be1111958d218542891e049d2bcb9da4e1a8"></a>
    <a id="t.3"></a>
    <table class="c43">
        <tbody>
            <tr class="c86">
                <td class="c84" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">Bit 7</span></p>
                </td>
                <td class="c62" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">Bit 6</span></p>
                </td>
                <td class="c70" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">Bit 5</span></p>
                </td>
                <td class="c36" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">Bit 4</span></p>
                </td>
                <td class="c36" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">Bit 3</span></p>
                </td>
                <td class="c46" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">Bit 2</span></p>
                </td>
                <td class="c46" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">Bit 1</span></p>
                </td>
                <td class="c46" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">Bit 0</span></p>
                </td>
            </tr>
            <tr class="c69">
                <td class="c84" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">XG_ST</span></p>
                </td>
                <td class="c62" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">YG_ST</span></p>
                </td>
                <td class="c70" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">ZG_ST</span></p>
                </td>
                <td class="c78" colspan="2" rowspan="1">
                    <p class="c5"><span class="c8">FS_SEL [1:0]</span></p>
                </td>
                <td class="c83" colspan="3" rowspan="1">
                    <p class="c5"><span class="c8">-</span></p>
                </td>
            </tr>
        </tbody>
    </table>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c10" id="h.ihv636"><span class="c8">Table 3.5: Full Scale Range Selection and Sensitivity</span></p>
    <a id="t.47a8e9dc52cfb0f83bcfe26f5b096fa3b83ca6ee"></a>
    <a id="t.4"></a>
    <table class="c43">
        <tbody>
            <tr class="c53">
                <td class="c19" colspan="1" rowspan="1">
                    <p class="c1"><span class="c13">AFS_SEL</span></p>
                </td>
                <td class="c27" colspan="1" rowspan="1">
                    <p class="c1"><span class="c13">Full Scale Range</span></p>
                </td>
                <td class="c33" colspan="1" rowspan="1">
                    <p class="c1"><span class="c13">LSB Sensitivity</span></p>
                </td>
            </tr>
            <tr class="c53">
                <td class="c19" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">0</span></p>
                </td>
                <td class="c27" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">&plusmn; 250 &deg;/s</span></p>
                </td>
                <td class="c33" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">131 LSB/&deg;/s</span></p>
                </td>
            </tr>
            <tr class="c53">
                <td class="c19" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">1</span></p>
                </td>
                <td class="c27" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">&plusmn; 500 &deg;/s</span></p>
                </td>
                <td class="c33" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">65.5 LSB/&deg;/s</span></p>
                </td>
            </tr>
            <tr class="c53">
                <td class="c19" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">2</span></p>
                </td>
                <td class="c27" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">&plusmn; 1000 &deg;/s</span></p>
                </td>
                <td class="c33" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">32.8 LSB/&deg;/s</span></p>
                </td>
            </tr>
            <tr class="c91">
                <td class="c19" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">3</span></p>
                </td>
                <td class="c27" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">&plusmn; 2000 &deg;/s</span></p>
                </td>
                <td class="c33" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">16.4 LSB/&deg;/s</span></p>
                </td>
            </tr>
        </tbody>
    </table>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c8">The most recent measurements of the gyroscope are stored in GYRO_XOUT_H, GYRO_XOUT_L, GYRO_YOUT_H, GYRO_YOUT_L, GYRO_ZOUT_H, and GYRO_ZOUT_L registers (Address: 43 - 48). The data within the gyroscope sensors&rsquo; internal register set is always updated at the Sample Rate selected. To get the actual measurement these 16 bit data needs to be divided by the LSB Sensitivity corresponding to the selected Full Scale Range. If a Full Scale range of &plusmn; 250 &deg;/s is selected then the actual gyroscope reading is given by the following equation.</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c5"><img src="images/image4.png"></p>
    <h3 class="c45 c82" id="h.32hioqz"><span class="c22 c75"></span></h3>
    <h3 class="c45" id="h.z2w26wusrn4c"><span class="c16">3.1.2 SEN-10264 Flex Sensor</span></h3>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c8">A Flex Sensor is a variable resistance. The resistance of the flex sensor changes as the sensor is flexed. The SEN-10264 Flex Sensor used in this project is 2.2&rdquo; flex sensor which is used to measure the flexing of fingers. As the sensor has a variable resistance it can be used in a voltage divider circuit to produce variable voltage output which can be then detected by the microcontroller [14]. The basic flex sensor circuit is shown on Figure 3.3 [15].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 417.50px; height: 222.07px;"><img alt="C:\Users\farha\AppData\Local\Microsoft\Windows\INetCache\Content.Word\flex-circuit.gif" src="images/image44.gif" style="width: 417.50px; height: 222.07px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c10" id="h.1hmsyys"><span class="c8">Figure 3.3: Basic flex sensor circuit</span></p>
    <p class="c5 c4"><span class="c8"></span></p>
    <p class="c5 c4"><span class="c8"></span></p>
    <p class="c10" id="h.41mghml"><span class="c8">Table 3.6: Electrical specifications of the sensor </span></p>
    <a id="t.fa28ca83690e1def7ef93aa79de6ffde029af0b9"></a>
    <a id="t.5"></a>
    <table class="c43">
        <tbody>
            <tr class="c15">
                <td class="c30" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">Parameters</span></p>
                </td>
                <td class="c64" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">Values</span></p>
                </td>
            </tr>
            <tr class="c15">
                <td class="c30" colspan="1" rowspan="1">
                    <p class="c1"><span class="c8">Flat Resistance</span></p>
                </td>
                <td class="c64" colspan="1" rowspan="1">
                    <p class="c1"><span class="c8">10k Ohms &plusmn;30%</span></p>
                </td>
            </tr>
            <tr class="c67">
                <td class="c30" colspan="1" rowspan="1">
                    <p class="c1"><span class="c8">Bend Resistance </span></p>
                </td>
                <td class="c64" colspan="1" rowspan="1">
                    <p class="c1"><span class="c8">Minimum 2 times greater than the flat resistance at 180&deg; pinch bend </span></p>
                </td>
            </tr>
        </tbody>
    </table>
    <p class="c1 c4"><span class="c2"><br></span></p>
    <h2 class="c45" id="h.2grqrue"><span class="c9">3.2 Microcontroller Description</span></h2>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21">&nbsp;&nbsp;&nbsp; </span><span class="c21 c26">The Microcontroller used in this project is an Esp32 (</span><span class="c12">low-power system on a chip microcontrollers with integrated Wi-Fi and dual-mode Bluetoot</span><span class="c8">h) based development board called DOIT Esp32 DevKit v1. This microcontroller was chosen instead of traditional AVR microcontrollers due to its higher ADC resolution, higher clock speed and wireless capabilities.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c3">DOIT Esp32 DevKit v1</span></p>
    <p class="c1 c4"><span class="c3"></span></p>
    <p class="c1"><span class="c21 c26">The DOIT Esp32 DevKit v1 is Esp32 based development board with </span><span class="c12">WiFi, Bluetooth, Ethernet and Low Power support. The general specifications of the board is given in Table 3.7</span></p>
    <p class="c1 c4"><span class="c2 c42"></span></p>
    <p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 593.50px; height: 304.36px;"><img alt="" src="images/image48.jpg" style="width: 593.50px; height: 304.36px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c5 c4"><span class="c8 c42"></span></p>
    <p class="c10" id="h.vx1227"><span class="c8 c42">Figure 3.4: Pinout diagram of </span><span class="c8">DOIT Esp32 DevKit v1</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c21 c26">Due to the higher clock speed and memory of the board this board can operate significantly faster than Arduino based microcontrollers. A speed test was performed between DOIT Esp32 DevKit v1 and an Arduino Uno board. The elapsed time to calculate median of 100 ADC values and mean of 1000 ADC values was used to compare the processing speed of both devices. The result shown in Figure 1.8 shows a significant difference in speed.</span></p>
    <p class="c10 c4"><span class="c2"></span></p>
    <p class="c10 c4"><span class="c2"></span></p>
    <p class="c10" id="h.3fwokq0"><span class="c8">Table 3.7: General Specifications of DOIT Esp32 DevKit v1</span></p>
    <a id="t.766122d7631311059b94cba605569a8f147064f0"></a>
    <a id="t.6"></a>
    <table class="c43">
        <tbody>
            <tr class="c0">
                <td class="c31" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13 c42">Parameters</span></p>
                </td>
                <td class="c65" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13 c42">Values</span></p>
                </td>
            </tr>
            <tr class="c0">
                <td class="c31" colspan="1" rowspan="1">
                    <p class="c5"><span class="c12">Operating Voltage</span></p>
                </td>
                <td class="c65" colspan="1" rowspan="1">
                    <p class="c5"><span class="c12">3.3 V</span></p>
                </td>
            </tr>
            <tr class="c0">
                <td class="c31" colspan="1" rowspan="1">
                    <p class="c5"><span class="c12">Clock Speed</span></p>
                </td>
                <td class="c65" colspan="1" rowspan="1">
                    <p class="c5"><span class="c12">240 MHz</span></p>
                </td>
            </tr>
            <tr class="c79">
                <td class="c31" colspan="1" rowspan="1">
                    <p class="c5"><span class="c12">Flash Memory</span></p>
                </td>
                <td class="c65" colspan="1" rowspan="1">
                    <p class="c5"><span class="c12">4 MB</span></p>
                </td>
            </tr>
            <tr class="c0">
                <td class="c31" colspan="1" rowspan="1">
                    <p class="c5"><span class="c12">SRAM</span></p>
                </td>
                <td class="c65" colspan="1" rowspan="1">
                    <p class="c5"><span class="c12">520 KB</span></p>
                </td>
            </tr>
            <tr class="c0">
                <td class="c31" colspan="1" rowspan="1">
                    <p class="c5"><span class="c12">Digital I/O Pins (DIO)</span></p>
                </td>
                <td class="c65" colspan="1" rowspan="1">
                    <p class="c5"><span class="c12">25</span></p>
                </td>
            </tr>
            <tr class="c0">
                <td class="c31" colspan="1" rowspan="1">
                    <p class="c5"><span class="c12">Analog Input Pins (ADC)</span></p>
                </td>
                <td class="c65" colspan="1" rowspan="1">
                    <p class="c5"><span class="c12">6</span></p>
                </td>
            </tr>
            <tr class="c0">
                <td class="c31" colspan="1" rowspan="1">
                    <p class="c5"><span class="c12">ADC Resolution</span></p>
                </td>
                <td class="c65" colspan="1" rowspan="1">
                    <p class="c5"><span class="c12">32 bits</span></p>
                </td>
            </tr>
            <tr class="c0">
                <td class="c31" colspan="1" rowspan="1">
                    <p class="c5"><span class="c12">WiFi</span></p>
                </td>
                <td class="c65" colspan="1" rowspan="1">
                    <p class="c5"><span class="c12">IEEE 802.11</span></p>
                </td>
            </tr>
        </tbody>
    </table>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c5"><span class="c21"><br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 511.00px; height: 393.96px;"><img alt="" src="images/image46.png" style="width: 511.00px; height: 393.96px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c10" id="h.1v1yuxt"><span class="c8">Figure 3.5: Speed comparison of two mcu for two different operation</span></p>
    <p class="c5 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c8">The DOIT Esp32 DevKit v1 also has a 12 bit ADC resolution compared to 10 bit resolution of the Arduino Uno. The relation between minimum measurable voltage by a microcontroller and the ADC resolution is given by the following equation.</span></p>
    <p class="c5"><img src="images/image5.png"></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c8">Arduino Uno has operating voltage of 5 volt and minimum measurable voltage is 4.88 mV whereas DOIT Esp32 DevKit v1 has operating voltage 3.3V and minimum measurable voltage is 0.806 mV.</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <h2 class="c45" id="h.4f1mdlm"><span class="c9">3.3 Dataset Description</span></h2>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c8">A group of 30 male and female volunteers aging between 21-32 years performed 14 hand gestures: come here, go away, fist, fingers crossed, cash, one, two, three, four, five, excellent, stop, thumbs up and thumbs down. 10 data were collected for each gestures using the data glove mounted with 5 flex sensors and 1 IMU modules containing 3-axis accelerometer and 3-axis gyroscope. Before taking the data, the data glove were calibrated by each volunteers themselves. This enabled us to normalize the flex sensor data for each participants and adjust the error of the IMU before recording data from each volunteers. Preprocessing of the IMU signals was done by using the digital motion procession (DMP) of the MPU-6050 chip. Random partition was also done for the obtained dataset where 70% of the enlistees were selected for generating the training data and 30% the test data.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <h2 class="c45" id="h.2u6wntf"><span class="c9">3.4 Features</span></h2>
    <p class="c1 c4"><span class="c22 c77"></span></p>
    <p class="c1"><span class="c8">Features are abstractions of raw data which represents the core characteristics and behaviors of the signal. Whereas, a feature vector is the reduced subset of large input data containing important hints for the activity to be recognized. Generally, there are three kinds of features in total:</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c8">1. Statistical domain or time domain features</span></p>
    <p class="c1"><span class="c8">2. Frequency domain or spectral domain features</span></p>
    <p class="c1"><span class="c8">3. Temporal domain features</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c8">The following Table shows many types of time, frequency and temporal domain features.</span></p>
    <p class="c4 c63" id="h.19c6y18"><span class="c2"></span></p>
    <p class="c10"><span class="c8">Table 3.8: List of different features</span></p>
    <a id="t.aebe33a6d7cd750d895adc95fb014422bfcdb9d4"></a>
    <a id="t.7"></a>
    <table class="c43">
        <tbody>
            <tr class="c15">
                <td class="c87" colspan="2" rowspan="1">
                    <p class="c5"><span class="c13">Features</span></p>
                </td>
            </tr>
            <tr class="c15">
                <td class="c37" colspan="1" rowspan="7">
                    <p class="c5"><span class="c8">Statistical Domain or Time Domain</span></p>
                </td>
                <td class="c55" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">Kurtosis</span></p>
                </td>
            </tr>
            <tr class="c15">
                <td class="c55" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">Skewness</span></p>
                </td>
            </tr>
            <tr class="c15">
                <td class="c55" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">Mean</span></p>
                </td>
            </tr>
            <tr class="c15">
                <td class="c55" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">Standard Deviation</span></p>
                </td>
            </tr>
            <tr class="c15">
                <td class="c55" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">Interquartile Range</span></p>
                </td>
            </tr>
            <tr class="c15">
                <td class="c55" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">Root Mean Square</span></p>
                </td>
            </tr>
            <tr class="c15">
                <td class="c55" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">Median Absolute Deviation</span></p>
                </td>
            </tr>
            <tr class="c15">
                <td class="c37" colspan="1" rowspan="3">
                    <p class="c5"><span class="c8">Temporal Domain</span></p>
                </td>
                <td class="c55" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">Zero Crossing Rate</span></p>
                </td>
            </tr>
            <tr class="c15">
                <td class="c55" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">Pairwise Correction</span></p>
                </td>
            </tr>
            <tr class="c15">
                <td class="c55" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">Autocorrelation</span></p>
                </td>
            </tr>
            <tr class="c15">
                <td class="c37" colspan="1" rowspan="5">
                    <p class="c5"><span class="c8">Frequency Domain</span></p>
                </td>
                <td class="c55" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">Maximum Frequency</span></p>
                </td>
            </tr>
            <tr class="c15">
                <td class="c55" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">Median Frequency</span></p>
                </td>
            </tr>
            <tr class="c15">
                <td class="c55" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">Power Spectrum</span></p>
                </td>
            </tr>
            <tr class="c15">
                <td class="c55" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">Power Bandwidth</span></p>
                </td>
            </tr>
            <tr class="c15">
                <td class="c55" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">Fundamental Frequency</span></p>
                </td>
            </tr>
        </tbody>
    </table>
    <h2 class="c45 c59" id="h.3tbugp1"><span class="c22 c75"></span></h2>
    <h2 class="c45"><span class="c9">3.6 Activity Recognition Based on Machine Learning Techniques</span></h2>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c8">Machine learning algorithms have become the most widely used approaches in activity recognition process based on the feature representation of data from an accelerometer alone or from both accelerometer and gyroscope [16]. Different assumptions are made by different machine learning algorithms about the shape and structure of the function and how best to optimize a representation to approximate it. There are mainly three types of machine learning algorithms: supervised, unsupervised and semi-supervised learning. In this project, we have chosen classification method under supervised learning to recognize the activities. We have built models using linear (assumed functional form is a linear combination of the input variables) or parametric (mapping is done to a known functional form), nonlinear or nonparametric (able to learn any mapping from input to output) and ensemble algorithms for training and we have also compared the accuracy of different algorithms. A short description of different algorithms we used is given below [17].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <h3 class="c45" id="h.28h4qwu"><span class="c16">3.6.1 Nonlinear or non-parametric algorithms</span></h3>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c8">These algorithms do not make strong assumptions about the form of mapping function and free to learn any functional form from the training data. In case of non-parametric methods, there are no much worries about choosing just the right features. These algorithms are capable of fitting a large number of functional forms, results in higher performance models for prediction and makes no assumptions (or weak assumptions) about the underlying function. </span></p>
    <p class="c1"><span class="c8">But these methods have some primary limitations as they require a lot more training data and for the mapping function estimation, a lot slower to train due to a large number of parameters to train and risk of overfitting the training data [16].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c3">Decision Tree (DT)</span></p>
    <p class="c1"><span class="c8">For predictive modeling machine learning, Decision Trees are an important type of algorithm. However, in case of decision trees, a non-linear relationship between predictors and outcome deteriorate the accuracy. Classification and Regression Trees (CART model) is represented by a binary tree. A single input variable(x) and a split point on that numeric variable are represented by each node of the tree. The leaf nodes of the tree contain an output variable (y), which is used to make a prediction where making a prediction is straightforward. The tree evaluates new input started at the root node of the tree whereas, a learned binary tree is regarded as a partitioning of the input space [16].</span></p>
    <p class="c1"><span class="c8">The complexity of a decision tree is based on the number of splits in the tree. Pruning method is used to further lift performance. However, accuracy will suffer, if there is a non-linear relationship between predictors and outcome.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c3">K-Nearest Neighbours (KNN)</span></p>
    <p class="c1"><span class="c8">KNN is one of the simplest algorithms as the entire training dataset is the model representation for KNN and predictions are made using the training dataset directly. No learning is required other than storing the entire dataset. Care must be taken about the consistency of training data and remove erroneous and outlier data. When a new data point arrives, predictions are made by searching through the entire dataset for K most similar instances (neighbors) and summarizing the output variable for those K instances [16].</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c3">Support Vector Machine (SVM)</span></p>
    <p class="c1"><span class="c8">Support Vector Machines are one of the most popular machine learning algorithms which can be explained by Maximal-Margin classifier [16]. In this case, an n-dimensional space is formed by the input variables of data. In SVM, a hyperplane is selected to best separate the points in the input variable space by their class. The distance between the hyperplane and the closest data points (support vectors) is called margin. Maximal-Margin hyperplane is the optimal line with the largest margin (perpendicular distance from the line to the closest points) that can separate all the classes. The hyperplane is learned from training data. It uses an optimization procedure that maximizes the margin.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c3">3.6.2 Ensemble algorithms</span></p>
    <p class="c1"><span class="c8">Ensemble methods make use of multiple learning algorithms so that better predictive performance can be obtained than any of the constituent learning algorithm alone [18]. A machine learning ensemble allows for a much more flexible formation to exist among those alternatives, though it consists of only a solid finite set of alternative models. Ensemble methods build a set of classifiers and then classify new data points by exerting a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting.</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c3">Random Forest (RnF)</span></p>
    <p class="c1"><span class="c8">Random Forest is the modern variation of classical Decision Tree algorithms. In case of Random Forest algorithm, the sub-trees are learned in such a way that the predictions from them have less or weak correlation. The learning algorithm is restricted to a random sample of features of which to search. For classification problems, at each split point, m number of features can be searched. A good default to find the value of m is [16]</span></p>
    <p class="c5"><img src="images/image6.png"></p>
    <p class="c1"><span class="c8">Where, p is the number of input variables.</span></p>
    <hr style="page-break-before:always;display:none;">
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c4 c6 title"><span class="c9"></span></p>
    <p class="c6 c4 title"><span class="c9"></span></p>
    <p class="c6 title"><span class="c18">Chapter 4</span></p>
    <h1 class="c52" id="h.nmf14n"><span class="c21 c32">Methodology</span></h1>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c8">For sensor based hand gesture classification the first and foremost thing needed is a well-made dataset. Once the dataset is prepared, the process of classifying hand gestures from can be summarized into these fundamental steps: pre-processing raw sensor data, feature extraction and classification. </span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <h2 class="c35" id="h.37m2jsg"><span class="c9">4.1 Dataset Creation</span></h2>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c8">We have created the dataset by taking 14 gestures, 10 times each, from 30 volunteers. Among the volunteers there were 9 female and 21 male volunteering from age group of 21-32 years. When recording the data, all the volunteers were seated. The data were recording in the following sequence.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <h3 class="c35" id="h.1mrcu09"><span class="c16">4.1.1 Calibrating Flex Sensors</span></h3>
    <p class="c1 c4"><span class="c22 c77"></span></p>
    <p class="c1"><span class="c8">After putting on the data glove, the microcontroller of the device was reset and flex sensor calibration is started for 7 seconds. During this time, an onboard blue LED is turned on for calibration time indication. To calibrate the sensors, the volunteers stretched their palm and then made a fist to bend and stretch the flex sensors to their maximum values. Maximum and minimum data for each flex sensors were recorded during this time and just after the calibration time, normalization factor for each sensors were determined. During data recording, this normalization factor is multiplied with the raw data, so that the whole dataset contains flex data in a certain range regardless of different users. After the calibration time, the onboard LED is turned off.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <h3 class="c35" id="h.46r0co2"><span class="c16">4.1.2 Initializing IMU</span></h3>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c8">In this step, the onboard blue LED is turned on again for 3 seconds. During this time the glove is kept still and level with the horizontal axis. We have used MPU-6050 chip which is a MEMS based IMU consists of one 3-axis accelerometer and one 3-axis gyroscope. According to our analysis, this IMU needs to be calibrated each time after initialization to get stable result and also a stabilizing time must be provided to the MEMS sensors to stabilize. The IMU settling time analysis is given below.</span></p>
    <p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 464.00px; height: 359.10px;"><img alt="https://lh6.googleusercontent.com/jOmPxliU4d1vgGYP5NztI9ku97MDaiX9UI6mMX8OYhDgTAKHzWhtuEXuCEJaxqwz1Pk_iiPHv5NQsw8yEtLoswEfKhBIGGMq69i90VwU-tChxpEVXOwRqTjgH0Y8pk8-unF3PpLV" src="images/image47.png" style="width: 464.00px; height: 359.10px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c10" id="h.2lwamvv"><span class="c2">Figure 4.1: Settling time visualization of MPU-6050</span></p>
    <p class="c1"><span class="c8">According to the figure above, we can observe that it stabilizes nearly 90% after 3 seconds. After this time, an error calculation takes place by averaging 200 values for each axis of both the sensors. Later, during the data recording, this error value is subtracted from each axis value to record a stable value. After the IMU initialization is completed, the onboard blue LED is turned off.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <h3 class="c35" id="h.111kx3o"><span class="c16">4.1.3 Data Recording</span></h3>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c8">In this step, the volunteers mimicked the 14 gestures one after another and each time, 10 data were recorded. The gestures we have worked with are static ones. So, the hand was kept still during each gesture data recording. The data glove, gestures and custom-made data recording software interface is shown in following figures.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 460.00px; height: 291.00px;"><img alt="" src="images/image49.png" style="width: 460.00px; height: 291.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c10" id="h.3l18frh"><span class="c8">Figure 4.2 Data glove and sensor placements</span></p>
    <p class="c5 c4"><span class="c2"></span></p>
    <p class="c5 c4"><span class="c2"></span></p>
    <p class="c5 c4"><span class="c2"></span></p>
    <p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 600.00px; height: 801.00px;"><img alt="" src="images/image50.png" style="width: 600.00px; height: 801.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c10" id="h.206ipza"><span class="c8">Figure 4.3 Picture of gestures</span></p>
    <p class="c10 c4" id="h.xytri1kxvngi"><span class="c8"></span></p>
    <p class="c10 c4" id="h.ckorc2eropzl"><span class="c8"></span></p>
    <p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 623.00px; height: 350.00px;"><img alt="" src="images/image51.png" style="width: 623.00px; height: 350.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c10" id="h.4k668n3"><span class="c8">Figure 4.4 Custom made desktop software UI for data collection </span></p>
    <p class="c5 c4"><span class="c2"></span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <h2 class="c35" id="h.2zbgiuw"><span class="c9">4.2 Pre-processing Data</span></h2>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c8">Any data or samples which are received from any sensor are called raw data. It is not a good idea to use direct raw data for further analysis, as the signal is made up of several components and there can be intrinsic noise components. So each component needs to be examined. Therefore, to ensure better result and precision, it is necessary to understand the nature of the signals produced by the sensors before the development of algorithms to interpret data recorded by the system. Normally, four filters namely median filter, Butterworth low-pass filter, discrete wavelet package shrinkage and Kalman filter are used to filter acceleration and gyroscope noise [21], [22], [23], [24].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <h3 class="c35" id="h.1egqt2p"><span class="c16">4.2.1 Noise reduction from flex sensors</span></h3>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c73"><span class="c8">The Flex sensors used in this project has a very high value of internal resistance. Due to this highly resistive nature it is very sensitive to parasitic resistance of the surrounding environment. As a result the raw readings from the flex sensors are highly fluctuating in nature. To solve this problem we have implemented two filtering approach while taking data from the sensor. These methods are discussed below.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c3">Mean Filter</span></p>
    <p class="c1"><span class="c8">Mean filter is used to reduce the effect of sudden changes in data. To apply mean filter we have sampled the flex sensor data every 10 ms and the calculated the mean value of consecutive 10 data using the following equation.</span></p>
    <p class="c5"><img src="images/image7.png"></p>
    <p class="c1 c40"><span class="c21 c26">Where n is the number of instances and x is the flex data for i</span><span class="c21 c26 c60">th</span><span class="c8">&nbsp;instance. In our case n = 10.</span></p>
    <p class="c1"><span class="c8">&nbsp;</span></p>
    <p class="c1"><span class="c3">Median Filter</span></p>
    <p class="c1"><span class="c8">Median is the value separating the higher half from the lower half from the data sample which might be a population or a probability distribution. For a dataset, it may be thought as the &ldquo;middle&rdquo; value. It is very helpful to eliminate the outliers from the dataset. Median filter was applied by taking 7 data at 10 ms sampling period and then calculating the median of them using the following equation [25].</span></p>
    <p class="c5"><img src="images/image8.png"></p>
    <p class="c1"><span class="c8">Comparison between Mean and Median Filter</span></p>
    <p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 534.50px; height: 413.66px;"><img alt="https://lh5.googleusercontent.com/4i09qral4U32UpWWK0LY7D66yHNLpJIOqUPAoxb7sMMZpy_fE9QQCrx9pz6mVzp5JUmsdiDhTdVterb4a1mgFkvrEPpO1191I3sWDovQYFDgxTnQZbbuLb26NlUs32EVpPBjK7nj" src="images/image52.png" style="width: 534.50px; height: 413.66px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c10" id="h.3ygebqi"><span class="c8">Figure 4.5: Comparison between mean and median filter</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c8">From the figure we can see that both mean and median filters are good for reducing noise. But median filter is more immune to random noise spikes.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <h3 class="c35" id="h.2dlolyb"><span class="c16">4.2.2 IMU Sensor Data Processing</span></h3>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c8">The three dimensional orientation of hand position can be defined by the corresponding yaw, pitch and roll angle. To calculate these angles we have utilized two IMU sensors: accelerometer and gyroscope. The accelerometer sensor of the MPU-6050 is sensitive to earth&rsquo;s gravitational force. The Roll and Pitch angles can be calculated by measuring the components of earth&rsquo;s gravitational acceleration along the x, y and z axis. But the Yaw angle cannot be calculated by only using the accelerometer. For classification of certain gestures like &lsquo;Five&rsquo; and &lsquo;Stop&rsquo; the measurement of yaw angle is vital. This is why we have we have also utilized the gyroscope sensor. A gyroscope can measure angular velocity along the three axis. By integrating these values we can calculate the angular displacements. So by measuring the angular displacement along the X-Y plane we can calculate the yaw angle. But both accelerometer and gyroscope sensors have their drawbacks. The accelerometer sensor is very sensitive to external noises which results in rapidly fluctuating data. So the raw data from accelerometer is not suitable from training. On the other hand data from gyroscope does not suffer from external forces or noises. But these data tends to drift over time as shown in Figure 4.3. We have implemented two filtering techniques to compensate these errors.</span></p>
    <p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 562.50px; height: 434.86px;"><img alt="https://lh5.googleusercontent.com/zpcGlkVafMkXO2rGWRdx5R4qiUVvtyEoPl_T3r9WI_t2bc8roDKnxeqet6cjqQRuDj7fkjHOFc7zDE3H9Tykk8MUAeIz0ZqCpm-kUE1Oq3Nc8XHnYO7ECjSGJnV4hcqiY0RoeCBL" src="images/image53.png" style="width: 562.50px; height: 434.86px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c10" id="h.sqyw64"><span class="c8">Figure 4.6: Gyroscope data drift without complementary filter</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c3">Complementary Filter</span></p>
    <p class="c1"><span class="c8">A complementary filter combines the accelerometer and gyroscope data to provide a reliable measurement. Since the gyroscope data is immune to external noises but tends to drift over time we can use a small portion of accelerometer data to eliminate gyroscope drift error on the long term. In this case we have combined 96% gyroscope data and 4% accelerometer data to to construct a complementary filter. This proportion is tuned by an iterative approach. Once tuned the filter can provide reliable measurement for gesture classification. The comparison between raw accelerometer, raw gyroscope and complementary filter data is shown in Figure 4.4.</span></p>
    <p class="c5"><img src="images/image9.png"></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 541.50px; height: 392.06px;"><img alt="https://lh6.googleusercontent.com/ZpPTxshmlIrI7Budv0LhhJ-_UqleCYWc-fXIcFYcWOOyQ5FHVhvIchsm0lIpngKai6a_yWljhacjTx1oQlsnkoV7Wvp8uIyyUNnXdARbud7GJdWCy1QRsqo6CXqXdPfgt2EV0bO4" src="images/image54.png" style="width: 541.50px; height: 392.06px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c10" id="h.3cqmetx"><span class="c8">Figure 4.7: Gyroscope raw data vs complementary filtered data for pitch angle</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c8">A drift in gyroscope data can be seen after 60 secs in Figure 4.5</span></p>
    <p class="c5 c4"><span class="c2"></span></p>
    <p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 549.50px; height: 425.42px;"><img alt="https://lh4.googleusercontent.com/V2gN6AG3bGWg7iOD8tMZPTOYl2BFTtm5mupVQePWNJUUYdLSaxTA9wQYp_J_BailgJIMtGg2Cx5fPaj79Xo_VPZUL00vSVHpLpLRQedLtSzzXDNtrKnOlb-delehrm3V2aykyyCb" src="images/image55.png" style="width: 549.50px; height: 425.42px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c5"><span class="c8">Figure 4.8: Comparison between non filtered data and filtered data</span></p>
    <p class="c1 c4"><span class="c20"></span></p>
    <p class="c1 c4"><span class="c20"></span></p>
    <p class="c1"><span class="c3">Digital Motion processor (DMP)</span></p>
    <p class="c1"><span class="c8">A drawback of the complementary filter is that it cannot fix the drift error of the yaw angle because yaw angle change has no effect on accelerometer readings. So readings from the accelerometer cannot be used for long term correction of yaw angle. A solution to this problem is to use the on board digital motion processor of the MPU-6050. The DMP fuses the accelerometer and gyroscope data to eliminate yaw drift. A comparison between complementary filter and DMP data is shown below in Figure 4.6 [12].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 507.00px; height: 224.59px;"><img alt="https://lh5.googleusercontent.com/JlCW9al646Vfh4WS-KFhFyeCbp2lov-XudWYtboidJ_OWIo_MYMtstlpwgRCXN5vCBmUpvAqHByZo8YiDJ2GIP3gxFlrqIpD7yb0HAZxygabEmo20KJ-A6dQOVvU-eMf7xTNA6ac" src="images/image56.jpg" style="width: 507.00px; height: 224.59px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c10" id="h.1rvwp1q"><span class="c8">Figure 4.9: Comparison between complementary filter and DMP</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <h2 class="c35" id="h.4bvk7pj"><span class="c9">4.3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Feature Extraction</span></h2>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c8">Feature extraction and selection has a strong influence on the outcome of perfect classification of activities. We have already filtered the noise from flex and IMU sensor data. These filtered data was then used to generate the features for training the algorithms.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <h3 class="c35" id="h.2r0uhxc"><span class="c16">4.3.1 Flex Sensor Feature Extraction</span></h3>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c8">First of all the raw sensor data were pre-processed using the filtering technique discussed above. Then these filtered values were normalized according to Equation 1.33. Later these normalized values were used as features. </span></p>
    <p class="c5"><img src="images/image10.png"></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c21 c26">Where </span><img src="images/image11.png"><span class="c21 c26">&nbsp;is the empirical is is mean and</span><img src="images/image12.png"><span class="c8">&nbsp;is the standard deviation.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c50"><span class="c16">4.3.2 MPU-6050 Feature extraction</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c8">The DMP of MPU-6050 outputs data in quaternion format. A general form of quaternion is given by the following equation.</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c5"><img src="images/image13.png"></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c12">Where </span><span class="c12 c14">w</span><span class="c12">, </span><span class="c12 c14">x</span><span class="c12">, </span><span class="c12 c14">y</span><span class="c12">, and </span><span class="c12 c14">z</span><span class="c12">&nbsp;are real numbers, and </span><span class="c12 c14">i</span><span class="c12">, </span><span class="c12 c14">j</span><span class="c12">, and </span><span class="c12 c14">k</span><span class="c12">&nbsp;are the fundamental </span><span class="c12 c14">quaternion units</span><span class="c12">.</span></p>
    <p class="c1"><span class="c12">To calculate the gravity vector from the quaternion </span><span class="c12 c14">Q </span><span class="c12">equation 1-3 are used.</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c5"><img src="images/image14.png"></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c5"><img src="images/image15.png"></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c5"><img src="images/image16.png"></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c12">We can also calculate the Euler angles using the equations.</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c5"><img src="images/image17.png"></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c5"><img src="images/image18.png"></p>
    <p class="c5 c4"><span class="c8"></span></p>
    <p class="c5"><img src="images/image19.png"></p>
    <p class="c1"><span class="c12">&nbsp;</span></p>
    <p class="c1"><span class="c12">A representation of quaternion angles and corresponding Euler angles is shown in Figure 4.7 [26].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 530.00px; height: 519.59px;"><img alt="https://lh3.googleusercontent.com/--JMQ6T_M7KLktU7gngIJ9za5DeEhr6Us5i2Zi-EU1TMnO54JvQdbVp5PTOajVAOt33cayAjyuyeukYlP9QJ_kzAyLI1KkjEpEeRyMnv_nkdynwvbRhIjRmVClhs6FnCgTZZmOJK" src="images/image27.png" style="width: 530.00px; height: 519.59px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c10" id="h.1664s55"><span class="c8 c42">Figure 4.10: Quaternion orientation and Euler orientation</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1 c4"><span class="c2 c42"></span></p>
    <p class="c1 c4"><span class="c2 c42"></span></p>
    <p class="c1 c4"><span class="c2 c42"></span></p>
    <p class="c1"><span class="c12">Finally the Yaw, Pitch and Roll angles can be calculated using the following equations.</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c5"><img src="images/image20.png"></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c5"><img src="images/image21.png"></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c5"><img src="images/image22.png"></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c12">In this project we have utilized the yaw, pitch and roll angle values as features along with the normalized flex sensor features. </span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <h2 class="c35" id="h.3q5sasy"><span class="c21 c93">4.4 Classification</span></h2>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c8">We have split our data into train and test sets where train set contains 70% of the data and test set contains the rest 30%. Then we have built models using linear or parametric (Logistic Regression, Linear Discriminant Analysis), nonlinear or nonparametric (k-Nearest Neighbors, Naive Bayes, Decision Trees, Support Vector Machine with linear and with linear and radial basis function kernels) and ensemble (Random Forest) algorithms for training to predict the activities precisely. The parameters of these classifiers were tuned to maximize the accuracy. Finally the performance of each classifier is evaluated and discussed in the &lsquo;Result and Analysis&rsquo; section.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c6 c4 title"><span class="c9"></span></p>
    <p class="c6 c4 title"><span class="c9"></span></p>
    <p class="c6 title"><span class="c18">Chapter 5</span></p>
    <h1 class="c52" id="h.25b2l0r"><span class="c21 c32">Result and Analysis</span></h1>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c8">In this section, we have evaluated the classification accuracy of the glove using different classification algorithms. We have trained K-Nearest Neighbours, Support Vector Machine, Random Forest and Decision Tree Algorithm on the dataset and compared the K-Fold Cross Validation score and Confusion Matrices.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <h2 class="c35" id="h.kgcv8k"><span class="c9">5.1 Evaluation Method</span></h2>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c8">The confusion matrix is a specific table layout that helps us to visualize the performance of a model. Each row denotes the occurrences in an actual class and each column denotes the occurrences in a predicted class. From this table, we can calculate the overall accuracy (ACC) using the following equation:</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c5"><img src="images/image23.png"></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c8">There are some binary class performance parameters we have calculated using following equations:</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c5"><img src="images/image24.png"></p>
    <p class="c5"><img src="images/image25.png"></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c5"><img src="images/image26.png"></p>
    <p class="c1 c4"><span class="c2"><br></span></p>
    <h2 class="c35" id="h.34g0dwd"><span class="c9">5.2 Overfitting and Underfitting</span></h2>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c8">If the estimator models the training data too well then it is known as overfitting. Overfitting occurs if the model learns the detail and noise in the training data. Impact of overfitting is that noises of the training data are picked up and learned as concepts by the model. The problems are that new data may not contain these noises. Therefore, these concepts may not be applied to new data.</span></p>
    <p class="c1"><span class="c8">Underfitting means that the model is unable to fit the data well enough. An underfitting machine learning model is not a proper model. Therefore, performance on a training data is poor.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c3">Solution to Overfitting and Underfitting</span></p>
    <p class="c1"><span class="c8">Overfitting and underfitting both can lead to poor performance of the model. In machine learning, overfitting is the most common problem. Evaluating the model and reporting results on the same dataset causes overfitting because the model will always make a more precise prediction of data that it has seen before. To counter this overfitting we have to test our model on unseen data. Therefore, to limit overfitting we can use two techniques to evaluate our models.</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <ol class="c90 lst-kix_list_1-0 start" start="1">
        <li class="c57"><span class="c8">We can use resampling techniques. K-fold cross-validation is the most resampling technique. In K-fold cross-validation, the training dataset is split into k subsets. Then the model is implemented considering only set as the testing set and rests as the training set for k times. Finally, we calculate the average accuracy of these k results.</span></li>
        <li class="c68"><span class="c8">We can split some data from training set or use completely different testing set to predict the accuracy. In this case, we can evaluate the learning models on the testing dataset to get an idea about the performance of the model on unseen data.</span></li>
    </ol>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <h2 class="c45 c59"><span class="c22 c75"></span></h2>
    <h2 class="c45" id="h.1jlao46"><span class="c9">5.3 Evaluation</span></h2>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21">&nbsp;&nbsp;&nbsp; </span><span class="c8">The classification accuracy was evaluated using confusion matrices and binary class performance parameters as shown in Figure 5.1. The results are shown below:</span></p>
    <p class="c81"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 294.50px; height: 220.09px;"><img alt="" src="images/image28.png" style="width: 294.50px; height: 220.09px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c10" id="h.43ky6rz"><span class="c8">Figure 5.1: Confusion matrix &ndash; some parameters for evaluation</span>
        <hr style="page-break-before:always;display:none;">
    </p>
    <h3 class="c35" id="h.2iq8gzs"><span class="c16">5.3.1 Performance Analysis using Confusion Matrix</span></h3>
    <h3 class="c45 c82"><span class="c13 c22"></span></h3>
    <p class="c1"><span class="c3">K-Nearest Neighbors Classifier (KNN)</span></p>
    <p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 555.06px; height: 474.50px;"><img alt="https://lh4.googleusercontent.com/TbolWjl8GLoXo1jLTRb0o0fLskEZdAgM1NImb7-3sC3aG02sLnMbAfQ14nGMwsANBfnhe5Ci8ANOSqDAl01tOojGKeE4yIOdcefFSruBikgcoM_1MYOwfWNLzUsMPyiVFn96uvGF" src="images/image29.png" style="width: 555.06px; height: 474.50px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c10" id="h.xvir7l"><span class="c8">Figure 5.2: Confusion matrix for KNN classifier</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c8">The optimal value found for K was 3. Looking at the confusion matrix we can see that KNN classifier has a very good accuracy in classifying the dataset.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <hr style="page-break-before:always;display:none;">
    <p class="c17 c4"><span class="c20"></span></p>
    <p class="c1"><span class="c3">Support Vector Machine Classifier (SVM)</span></p>
    <p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 563.00px; height: 478.79px;"><img alt="https://lh4.googleusercontent.com/dEnzlAHuGYYiRlDSU7yeYtJKUgp2x75X9oXeuQUOw2NmLTdzeWusgTlxGXdjnpKMd9GNMOQI6aJ8UzUkGEJ13Ep2x_3j-1F8BMwwO5ptSglo2DmbVN6EjBTDEfLFnz5Nkp3RHUKV" src="images/image30.png" style="width: 563.00px; height: 478.79px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c5 c4"><span class="c2"></span></p>
    <p class="c10" id="h.3hv69ve"><span class="c8">Figure 5.3: Confusion matrix for SVM classifier</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c12">Radial basis function kernel (RBF) was used for the SVM classifier. The classifier works decently but not as good as the KNN classifier. From the confusion we can see that it faces problem classifying &lsquo;Come Here&rsquo;, &lsquo;Five&rsquo;, &lsquo;Fist&rsquo; and &lsquo;Stop&rsquo; gestures. </span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <hr style="page-break-before:always;display:none;">
    <p class="c17 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c3">Random Forest Classifier (RnF)</span></p>
    <p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 569.00px; height: 483.21px;"><img alt="https://lh4.googleusercontent.com/BYXYeaFPXtp5Nhcv5lVGBVzIb3LwRBBHc9a2XXHd5xR_VuhulQwOGPb9j_gugs4Ke9H9cZeR_FgbHvVuMqF6Bl6JPy_fasq2eMyfyN8wcKTrJg6S5XCglUoJmjCHLWSLb1WSva14" src="images/image31.png" style="width: 569.00px; height: 483.21px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c10" id="h.1x0gk37"><span class="c8">Figure 5.4: Confusion matrix for RnF Classifier</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1"><span class="c8">For RnF classifier parameters, n_estimator = 100, max_depth = 7 were found to be optimal. Again from the confusion matrix we can see that the RnF classifier is very much accurate in classifying the dataset. But faces hard time classifying the &lsquo;Come Here&rsquo; gesture.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <hr style="page-break-before:always;display:none;">
    <p class="c17 c4"><span class="c20"></span></p>
    <p class="c1"><span class="c3">Decision Tree Classifier (DT)</span></p>
    <p class="c1"><span class="c8">&nbsp;&nbsp;&nbsp; DT classifier performs very well like KNN classifier. From the confusion matrix we can see that it is very accurate in classifying the dataset.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 540.00px; height: 470.42px;"><img alt="https://lh3.googleusercontent.com/0XJnyxG2hZGRB-i1VW1pRw9sI595LJvwg9_1w7_voL05d0P_E16_LpusEuiKqycrvEby5xBVwUbLYzktLyoTYRtwX-dCMmBNRrc-NPjiNxI6r0e3y4uUMVESvUxq-46slbHiWtBA" src="images/image32.png" style="width: 540.00px; height: 470.42px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c10" id="h.4h042r0"><span class="c8">Figure 5.5: Confusion matrix for DT classifier</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <hr style="page-break-before:always;display:none;">
    <p class="c17 c4"><span class="c20"></span></p>
    <h3 class="c35" id="h.2w5ecyt"><span class="c16">5.3.2 Overall Accuracy</span></h3>
    <p class="c5 c4"><span class="c2"></span></p>
    <p class="c10" id="h.1baon6m"><span class="c8">Table 5.1: Overall Accuracy of the Classifiers</span></p>
    <a id="t.bf0d2c17fa588d0281170de95cf6a8ef383f22d8"></a>
    <a id="t.8"></a>
    <table class="c43">
        <tbody>
            <tr class="c53">
                <td class="c39" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">Classifier</span></p>
                </td>
                <td class="c85" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">KNN (K=3)</span></p>
                </td>
                <td class="c92" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">SVM (RBF)</span></p>
                </td>
                <td class="c76" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">RnF (Depth = 7)</span></p>
                </td>
                <td class="c88" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">DT</span></p>
                </td>
            </tr>
            <tr class="c53">
                <td class="c39" colspan="1" rowspan="1">
                    <p class="c5"><span class="c13">Accuracy</span></p>
                </td>
                <td class="c85" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">99.53%</span></p>
                </td>
                <td class="c92" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">94.97%</span></p>
                </td>
                <td class="c76" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">97.48%</span></p>
                </td>
                <td class="c88" colspan="1" rowspan="1">
                    <p class="c5"><span class="c8">98.74%</span></p>
                </td>
            </tr>
        </tbody>
    </table>
    <p class="c5 c4"><span class="c2"><br></span></p>
    <h3 class="c35" id="h.3vac5uf"><span class="c16">5.3.3 Binary Classification Performances </span></h3>
    <p class="c1"><span class="c2">&nbsp;&nbsp;&nbsp; </span></p>
    <p class="c1"><span class="c8">Here we compare the Precision, Recall (Sensitivity) and Balanced Classifier Rate (BCR) values for each of the four classifier used.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c50"><span class="c3">5.3.3.1 Precision and Sensitivity</span></p>
    <p class="c1"><span class="c3">KNN Classifier</span></p>
    <p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 523.00px; height: 261.50px;"><img alt="https://lh5.googleusercontent.com/cMFJ5RP11-N-jjubJ6_M2aiNAPNbjb3UFi9SICiiSECg33Y0CA8tlykJN1YsR7w88oOHiVVvSXGgzmKt0ne68RhW6qjr14YoaKRoUgQLqQlKRThxHAhjfMGiYG8zv6Aitfu23Pkq" src="images/image33.png" style="width: 523.00px; height: 261.50px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c5"><span class="c18 c26">Figure 5.6: Precision and Sensitivity for KNN</span></p>
    <p class="c1 c4"><span class="c20"></span></p>
    <p class="c1"><span class="c3">SVM Classifier</span></p>
    <p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 531.00px; height: 265.02px;"><img alt="https://lh6.googleusercontent.com/ALscIDAeMwYiKL52OBQM1mfvJmKOaQNONSHIitUCbeYPj3pL6D7iBmxGvTcrWhfFg8C-rDW8drJbNkmm6K5h6Kac50xv_gUBUwrZ2o5I7JG4FgazbqG32SBNRwuRN9BYkwZShjOB" src="images/image34.png" style="width: 531.00px; height: 265.02px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c10" id="h.2afmg28"><span class="c8">Figure 5.7: Precision and Sensitivity for SVM</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c3">Random Forest Classifier</span></p>
    <p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 520.00px; height: 260.49px;"><img alt="https://lh3.googleusercontent.com/0oFRop36E47C75ZYt5LY_mqrrVtPVkAV9ikz5oOYLEUTRv31Y5Hemx2nNlfMnrII-Xxg0B0T9H9r8-BzXJUcaR3ei8eAxv5zZAPZnyQyZJ2SfywXJ0nTdtXGt0dv8QxRbMVnwu1S" src="images/image35.png" style="width: 520.00px; height: 260.49px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c10" id="h.pkwqa1"><span class="c8">Figure 5.8: Precision and Sensitivity for RnF</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1 c4"><span class="c20"></span></p>
    <p class="c1 c4"><span class="c20"></span></p>
    <p class="c1"><span class="c3">Decision Tree Classifier</span></p>
    <p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 532.50px; height: 266.25px;"><img alt="https://lh6.googleusercontent.com/pVW1zoDHokZGU-jPiLbJ2QkDA3qECFyL4BPn2IGkoCC11U9BbPFIn4VN33D_92RlephYubmicXekuHLgAmu16cZMckArcSPcqgd_MX7gD0KtcEyon7lFWCuZMrMm1MPeQOaILlB3" src="images/image36.png" style="width: 532.50px; height: 266.25px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c10" id="h.39kk8xu"><span class="c8">Figure 5.9: Precision and Sensitivity for DT</span></p>
    <p class="c10 c4" id="h.qtxxunbqguzw"><span class="c8"></span></p>
    <p class="c50"><span class="c3">5.3.3.2 Balanced Classifier Rate</span></p>
    <p class="c1"><span class="c8">Balanced Classification Rate or BCR combines the Precision and Sensitivity values into one single quantity. The comparison of BCR values for the classifiers are given below.</span></p>
    <p class="c5" id="h.1opuj5n"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 390.68px; height: 301.19px;"><img alt="https://lh5.googleusercontent.com/Ung_FXZMdXosmXmKabNmCfujNXuB8HLH2lTrDyHlXvInVK2dcGxxIo-RB8TL2RnQoosgddVqYtcGfK6irqFnOZoitJBOlJgohxxcOVJzR2i112HV687iAVD-kVUFvPubFcfqOYyg" src="images/image43.png" style="width: 390.68px; height: 301.19px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
    <p class="c10" id="h.48pi1tg"><span class="c8">Figure 5.10: BCR of the four classifiers</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <h2 class="c35" id="h.2nusc19"><span class="c9">5.4 Discussion</span></h2>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21 c26">From the above evaluation methods we can conclude that all these four classifiers have shown over 90% classification. SVM classifier</span><span class="c12">&nbsp;faces problem classifying &lsquo;Come Here&rsquo;, &lsquo;Five&rsquo;, &lsquo;Fist&rsquo; and &lsquo;Stop&rsquo; gestures and RnF </span><span class="c21 c26">classifier faces hard time classifying the &lsquo;Come Here&rsquo; gesture. Decision Tree shows quite good accuracy and does not face crucial complicacy for any particular gesture. KNN (k=3) classifier shows the best overall classification accuracy of 99.53%. So, we can conclude that KNN classifier is preferred for practical gesture recognition application using this method.</span>
        <hr style="page-break-before:always;display:none;">
    </p>
    <p class="c6 c4 title"><span class="c9"></span></p>
    <p class="c6 c4 title"><span class="c9"></span></p>
    <p class="c6 title"><span class="c18">Chapter 6</span></p>
    <h1 class="c52" id="h.1302m92"><span class="c21 c32">Conclusion and future work</span></h1>
    <p class="c1 c4"><span class="c20"></span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c8">The main contribution of this work is to establish a framework for hand gesture recognition using sensor based approach. There has been a number of sensor based gesture recognition work based on a custom made data glove and different general purpose devices i.e. wiimote, apple watch, mi band and even smartphones which contains various sensors. These general purpose devices does not fulfill the requirements in our methods. For example, our key sensors are the flex sensors and the accelerometers but the general purpose devices does not contain flex sensors. So we had to make our own robust data glove containing these sensors. As the device is custom-made, there is no previous dataset which matches this device utility. So, we have built our own dataset and trained four different classifiers. The result has shown high accuracy of detection for the gestures we chose. The other data glove of the previous works have mostly used accelerometer data for hand gesture detection. We have worked with the accelerometer, gyroscope and flex sensor data which has given this framework a wide range of gesture detection capability. </span></p>
    <p class="c1"><span class="c8">Moreover we have made a comparative analysis of different noise reduction techniques. We have examined several works regarding accelerometer and gyroscope noise reduction to get stable orientation data out of it. Moreover, we have also faced several IMU data extraction challenges such as drift in gyroscope data, high frequency noise reduction, outlier cancellation and gimbal lock.</span></p>
    <p class="c1"><span class="c8">Besides our proposed framework is only focused in classification of static gestures which exclude a lot of natural gestures in day to day life. Another limitation of this framework is the need of a computer to process the classifier. So, this data glove is not yet applicable to use in places where there is no communication available with the central processing computer.<br><br></span></p>
    <p class="c1"><span class="c9">Future Work</span></p>
    <p class="c1"><span class="c13"><br></span><span class="c8">In future, we intend to improve the dataset by taking data from more volunteers and also we would like to apply Kalman-filter to further noise reduction of the IMU. We would like to improve the framework by making it independent of the computer to detect gestures. Moreover, we would like to use this proposed technique to classify Bangla sign language gestures.</span></p>
    <p class="c1 c4"><span class="c8"></span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <hr style="page-break-before:always;display:none;">
    <p class="c4 c17"><span class="c2"></span></p>
    <h1 class="c52" id="h.3mzq4wv"><span class="c22 c21 c32">Bibliography</span></h1>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21">[1] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;What is Human-Computer Interaction (HCI)?&quot;, </span><span class="c11">The Interaction Design Foundation</span><span class="c2">, 2019. [Online]. Available: https://www.interaction-design.org/literature/topics/human computer-interaction. [Accessed: 13- May- 2019].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21">[2]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; H. Kaur and J. Rani, &quot;A review: Study of various techniques of Hand gesture recognition,&quot; </span><span class="c11">2016 IEEE 1st International Conference on Power Electronics, Intelligent Control and Energy Systems (ICPEICES)</span><span class="c2">, Delhi, 2016, pp. 1-5.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21">[3] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;J. S. Sonkusare, N. B. Chopade, R. Sor and S. L. Tade, &quot;A Review on Hand Gesture Recognition System,&quot; </span><span class="c11">2015 International Conference on Computing Communication Control and Automation</span><span class="c2">, Pune, 2015, pp. 790-794.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21">[4] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;M. Banko and E. Brill, &quot;Scaling to very very large corpora for natural language disambiguation&quot;, </span><span class="c11">Proceedings of the 39th Annual Meeting on Association for Computational Linguistics - ACL &#39;01</span><span class="c2">, 2001. Available: 10.3115/1073012.1073017 [Accessed 12 May 2019].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21">[5] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;J. M&auml;ntyj&auml;rvi, J. Kela, P. Korpip&auml;&auml; and S. Kallio, &quot;Enabling fast and effortless customisation in accelerometer based gesture interaction&quot;, </span><span class="c11">Proceedings of the 3rd international conference on Mobile and ubiquitous multimedia - MUM &#39;04</span><span class="c2">, 2004. Available: 10.1145/1052380.1052385 [Accessed 13 May 2019].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21">[6] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;T. Schl&ouml;mer, B. Poppinga, N. Henze and S. Boll, &quot;Gesture recognition with a Wii controller&quot;, </span><span class="c11">Proceedings of the 2nd international conference on Tangible and embedded interaction - TEI &#39;08</span><span class="c2">, 2008. Available: 10.1145/1347390.1347395 [Accessed 13 May 2019].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21">[7] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;S. Roy, S. Ghosh, A. Barat, M. Chattopadhyay and D. Chowdhury, &quot;Real-time Implementation of Electromyography for Hand Gesture Detection Using Micro Accelerometer&quot;, </span><span class="c11">Advances in Intelligent Systems and Computing</span><span class="c2">, pp. 357-364, 2016. Available: 10.1007/978-81-322-2656-7_32 [Accessed 13 May 2019].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21">[8] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Xu Zhang, Xiang Chen, Yun Li, V. Lantz, Kongqiao Wang and Jihai Yang, &quot;A Framework for Hand Gesture Recognition Based on Accelerometer and EMG Sensors&quot;, </span><span class="c11">IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans</span><span class="c2">, vol. 41, no. 6, pp. 1064-1076, 2011. Available: 10.1109/tsmca.2011.2116004 [Accessed 13 May 2019].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21">[9] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;C. Amma, D. Gehrig and T. Schultz, &quot;Airwriting recognition using wearable motion sensors&quot;, </span><span class="c11">Proceedings of the 1st Augmented Human International Conference on - AH &#39;10</span><span class="c2">, 2010. Available: 10.1145/1785455.1785465 [Accessed 13 May 2019].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21">[10]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; C. Pittman, P. Wisniewski, C. Brooks and J. LaViola, &quot;Multiwave&quot;, </span><span class="c11">Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems - CHI EA &#39;16</span><span class="c2">, 2016. Available: 10.1145/2851581.2892286 [Accessed 13 May 2019].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21">[11] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;J. Uddin, F. Arko, N. Tabassum, T. Trisha and F. Ahmed, &quot;Bangla sign language interpretation using bag of features and Support Vector Machine&quot;, </span><span class="c11">2017 3rd International Conference on Electrical Information and Communication Technology (EICT)</span><span class="c2">, 2017. Available: 10.1109/eict.2017.8275173 [Accessed 13 May 2019].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21">[12]&quot;How to Interface Arduino and the MPU 6050 Sensor | Arduino&quot;, </span><span class="c11">Maker Pro</span><span class="c2">, 2019. [Online]. Available: https://maker.pro/arduino/tutorial/how-to-interface-arduino-and-the-mpu-6050-sensor. [Accessed: 16- May- 2019].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21">[13]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;MPU-6050 | TDK&quot;, </span><span class="c11">Invensense.com</span><span class="c2">, 2019. [Online]. Available: https://www.invensense.com/products/motion-tracking/6-axis/mpu-6050/. [Accessed: 16- May- 2019].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21">[14] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;Flex Sensor Hookup Guide - learn.sparkfun.com&quot;, </span><span class="c11">Learn.sparkfun.com</span><span class="c2">, 2019. [Online]. Available: https://learn.sparkfun.com/tutorials/flex-sensor-hookup-guide/all. [Accessed: 13- May- 2019].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21">[15]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c11">Cdn.sparkfun.com</span><span class="c2">, 2019. [Online]. Available: https://cdn.sparkfun.com/datasheets /Sensors/ForceFlex/FLEX%20SENSOR%20DATA%20SHEET%202014.pdf. [Accessed: 16- May- 2019].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21">[16] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Jason Brownlee. </span><span class="c11">Master machine learning algorithm: Discover how they work and implement them from scratch, </span><span class="c2">2016.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21">[17]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I. Witten and E. Frank, </span><span class="c11">Data Mining: Practical Machine Learning Tools and Techniques</span><span class="c2">. San Francisco, Calif.: Morgan Kaufmann, 2011.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21">[18]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;D. Opitz and R. Maclin, &quot;Popular Ensemble Methods: An Empirical Study&quot;, </span><span class="c11">Journal of Artificial Intelligence Research</span><span class="c2">, vol. 11, pp. 169-198, 1999. Available: 10.1613/jair.614.</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21">[19]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;J. Wu, G. Pan, D. Zhang, G. Qi and S. Li, &quot;Gesture Recognition with a 3-D Accelerometer&quot;, </span><span class="c11">Ubiquitous Intelligence and Computing</span><span class="c2">, pp. 25-38, 2009. Available: 10.1007/978-3-642-02830-4_4 [Accessed 16 May 2019].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21">[20]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c11">Invensense.com</span><span class="c2">, 2019. [Online]. Available: https://www.invensense.com/wp-content/uploads/2015/02/MPU-6000-Register-Map1.pdf. [Accessed: 16- May- 2019].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c2">[21]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;M. J. Mathie. Monitoring and interpreting human movement patterns using a</span></p>
    <p class="c1"><span class="c21">triaxial accelerometer</span><span class="c11">. Ph.D. thesis, Univ. New South Wales, Sydney</span><span class="c2">, August 2003.</span></p>
    <p class="c1"><span class="c21">[22]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;M. Huang, G. Zhao, L. Wang and F. Yang, &quot;A Pervasive Simplified Method for Human Movement Pattern Assessing&quot;, </span><span class="c11">2010 IEEE 16th International Conference on Parallel and Distributed Systems</span><span class="c2">, 2010. Available: 10.1109/icpads.2010.65 [Accessed 16 May 2019].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21">[23]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;L. Rong, Z. Jianzhong, L. Ming and H. Xiangfeng, &quot;A Wearable Acceleration Sensor System for Gait Recognition&quot;, </span><span class="c11">2007 2nd IEEE Conference on Industrial Electronics and Applications</span><span class="c2">, 2007. Available: 10.1109/iciea.2007.4318894 [Accessed 16 May 2019].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c2">[24]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Tiexiang Wen, Lei Wang, Jia Gu, and Bangyu Huang. An acceleration-based control framework for interactive gaming. IEEE 31st annual conference of EMBS, Minnesota, 2009 pages 2388&ndash;2391, Sept. 3-6, 2009 2009.</span></p>
    <p class="c50 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21">[25]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;Median&quot;, </span><span class="c11">En.wikipedia.org</span><span class="c2">, 2019. [Online]. Available: https://en.wikipedia .org/wiki/Median. [Accessed: 16- May- 2019].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1"><span class="c21">[26]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;P. Scheidler, &quot;Quaternions for Orientation&quot;, </span><span class="c11">Blog.mide.com</span><span class="c2">, 2019. [Online]. Available: https://blog.mide.com/quaternions-for-orientation. [Accessed: 16- May- 2019].</span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <hr style="page-break-before:always;display:none;">
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1 c4"><span class="c2"></span></p>
    <p class="c1 c4"><span class="c2"></span></p>
</div>
</body>

</html>